[
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Content",
    "section": "",
    "text": "Week 1 - Introduction to RMDA\n\n\n\n\n\n\nFelipe Melo\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Data Wrangling & Research Questions\n\n\n\n\n\n\nFelipe Melo\n\n\nApr 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Data Visualisation & Scientific Hypotheses\n\n\n\n\n\n\nFelipe Melo\n\n\nApr 19, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "RM&DA Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nGet-started  Here is where you should start your journey. This is a short introduction to R and RStudio.\nContent  All the lessons live here, grouped by weeks.\n\n[Preparation] - Every week starts here with a few steps to set up the basics in terms of packages and readings.\n[Session] - Here you’ll find the slides and recorded videos focused on the core learning outcomes\n[Exercise] - This is your post-sesssion, auto-learning hours of study and must be organized in your Quarto workbook.\n\n\n\n\n\n\ntl;dr: You should follow this general process for each week:\n\nDo everything on the preparation section ()\nWatch the recorded videos and check the slides of the lesson ()\nComplete the exercise () using your quarto workbook () and submit it to the respective Dropbox"
  },
  {
    "objectID": "schedule.html#schedule",
    "href": "schedule.html#schedule",
    "title": "RM&DA Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nGet-started  Here is where you should start your journey. This is a short introduction to R and RStudio.\nContent  All the lessons live here, grouped by weeks.\n\n[Preparation] - Every week starts here with a few steps to set up the basics in terms of packages and readings.\n[Session] - Here you’ll find the slides and recorded videos focused on the core learning outcomes\n[Exercise] - This is your post-sesssion, auto-learning hours of study and must be organized in your Quarto workbook.\n\n\n\n\n\n\ntl;dr: You should follow this general process for each week:\n\nDo everything on the preparation section ()\nWatch the recorded videos and check the slides of the lesson ()\nComplete the exercise () using your quarto workbook () and submit it to the respective Dropbox"
  },
  {
    "objectID": "get-started/1-install.html",
    "href": "get-started/1-install.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "get-started/1-install.html#video-tutorials",
    "href": "get-started/1-install.html#video-tutorials",
    "title": "Installing R and RStudio",
    "section": "Video Tutorials",
    "text": "Video Tutorials"
  },
  {
    "objectID": "sessions/week1/1-intro.html",
    "href": "sessions/week1/1-intro.html",
    "title": "Week 1 - Introduction to RMDA",
    "section": "",
    "text": "Today you are going to understand how this modules works."
  },
  {
    "objectID": "sessions/week1/1-intro.html#slides",
    "href": "sessions/week1/1-intro.html#slides",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Slides",
    "text": "Slides\n    View slides in full screen"
  },
  {
    "objectID": "sessions/week1/1-intro.html#videos",
    "href": "sessions/week1/1-intro.html#videos",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Videos",
    "text": "Videos"
  },
  {
    "objectID": "sessions/week1/1-intro.html#exercise-1---create-your-workbook",
    "href": "sessions/week1/1-intro.html#exercise-1---create-your-workbook",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Exercise 1 - Create your workbook",
    "text": "Exercise 1 - Create your workbook"
  },
  {
    "objectID": "sessions/week1/1-intro.html#introduction",
    "href": "sessions/week1/1-intro.html#introduction",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Introduction",
    "text": "Introduction\nQuarto is a powerful tool for creating dynamic documents and websites. This tutorial will guide you through the process of creating a simple webpage using Quarto."
  },
  {
    "objectID": "sessions/week1/1-intro.html#prerequisites",
    "href": "sessions/week1/1-intro.html#prerequisites",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you begin, make sure you have the following installed:\n\nQuarto: You can download and install Quarto from quarto.org.\nRstudio: However, any text editor will work, such as VS Code, Sublime Text, or Notepad++.\n\n\nStep 1: Create a New Quarto Document\n\nOpen your terminal or command prompt.\nNavigate to the directory where you want to create your webpage.\nRun the following command to create a new Quarto document:\nquarto create webpage.qmd\nThis command creates a file named webpage.qmd.\n\n\n\nStep 2: Edit the Quarto Document\n\nOpen webpage.qmd in your text editor.\nYou’ll see some default content, including a title and some example text.\nModify the content to create your webpage. For example:\n---\ntitle: \"My First Quarto Webpage\"\nformat: html\n---\n\n## Welcome!\n\nThis is my first webpage created with Quarto.\n\nHere's a simple list:\n\n* Item 1\n* Item 2\n* Item 3\n\nYou can also include code blocks:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(\"Hello, Quarto!\")\n\n[1] \"Hello, Quarto!\"\n\n:::\nAnd mathematical equations:\n\\[\nE = mc^2\n\\]\nYou can add images too:\n\nknitr::include_graphics(\"https://quarto.org/quarto.png\")\n\n\n\n\n\n\n\n\n```\nExplanation of the code:\n\n--- title: \"My First Quarto Webpage\" format: html --- : This is the YAML header, which sets the title of your webpage and specifies the output format (HTML in this case).\n## Welcome! : This is a level 2 heading.\n* Item 1 : This creates a bulleted list.\n```{r}: This begins a R code block.\n$$E = mc^2$$: This inserts a LaTeX equation.\n```{r}: This begins an R code block.\nknitr::include_graphics(\"https://quarto.org/quarto.png\"): This includes an image from a URL.\n\n\n\n\nStep 3: Render the Webpage\n\nIn your terminal or command prompt, navigate to the directory containing webpage.qmd.\nRun the following command to render the webpage:\nquarto render webpage.qmd\nThis command will create an HTML file named webpage.html in the same directory.\n\n\n\nStep 4: View the Webpage\n\nOpen webpage.html in your web browser.\nYou should see your webpage with the content you created."
  },
  {
    "objectID": "sessions/week1/1-intro.html#adding-more-content",
    "href": "sessions/week1/1-intro.html#adding-more-content",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Adding more content",
    "text": "Adding more content\nYou can add more content to your webpage by editing webpage.qmd. Quarto supports various types of content, including:\n\nHeadings: Use #, ##, ###, etc. for different heading levels.\nParagraphs: Just type your text.\nLists: Use * or - for unordered lists, and 1., 2., etc. for ordered lists.\nCode Blocks: Use ```{language} to insert code blocks.\nMathematical Equations: Use $$ for LaTeX equations.\nImages: Use ![alt text](path/to/image.png) or {r} knitr::include_graphics(\"path/to/image.png\").\nLinks: Use [link text](url).\nTables: Use Markdown table syntax."
  },
  {
    "objectID": "sessions/week1/1-intro.html#further-exploration",
    "href": "sessions/week1/1-intro.html#further-exploration",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Further Exploration",
    "text": "Further Exploration\n\nExplore the Quarto documentation for more advanced features: quarto.org.\nExperiment with different output formats, such as PDF or Word.\nLearn about Quarto projects for creating multi-page websites.\nLook into adding CSS and Javascript for styling and interactivity."
  },
  {
    "objectID": "sessions/week1/1-intro.html#publishing-to-quarto-pub",
    "href": "sessions/week1/1-intro.html#publishing-to-quarto-pub",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Publishing to Quarto Pub",
    "text": "Publishing to Quarto Pub\nKnow how to publish your Quarto webpage to Quarto Pub, making it accessible online."
  },
  {
    "objectID": "sessions/week1/1-intro.html#prerequisites-1",
    "href": "sessions/week1/1-intro.html#prerequisites-1",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAll the prerequisites from the previous tutorial.\nA Quarto Pub account. You can create one at quarto.pub.\n\n\nStep 1: Create a Quarto Webpage (If you haven’t already)\nIf you haven’t already created a Quarto webpage, follow the steps in the previous tutorial to create webpage.qmd and render it into webpage.html.\n\n\nStep 2: Initialize Quarto Pub\n\nOpen your terminal or command prompt.\nNavigate to the directory containing webpage.qmd.\nRun the following command to initialize Quarto Pub:\nquarto publish quarto-pub\nThis command will prompt you to log in to your Quarto Pub account. Follow the instructions to authenticate.\n\n\n\nStep 3: Publish Your Webpage\n\nAfter successful authentication, Quarto will detect the webpage.html file and ask you if you want to publish it.\nConfirm that you want to publish the webpage.\nQuarto will upload your webpage to Quarto Pub.\nYou’ll receive a URL where your webpage is hosted.\n\n\n\nStep 4: View Your Published Webpage\n\nOpen the URL provided by Quarto in your web browser.\nYou should see your webpage hosted on Quarto Pub.\n\n\n\nStep 5: Updating your published page.\nIf you edit the webpage.qmd file, you will need to re-render the html file, and then republish.\n\nEdit webpage.qmd with your text editor.\nRender the html file again.\nquarto render webpage.qmd\nRepublish the webpage.\nquarto publish quarto-pub\nQuarto Pub will update the existing webpage with the new content."
  },
  {
    "objectID": "sessions/week1/1-intro.html#important-considerations",
    "href": "sessions/week1/1-intro.html#important-considerations",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nFile Organization: For more complex websites, consider creating a Quarto project. This will help you organize your files and manage your website more effectively.\nCustom Domains: Quarto Pub allows you to use custom domains for your websites. Refer to the Quarto Pub documentation for instructions.\nSecurity: Be mindful of the content you publish online. Avoid sharing sensitive information.\nQuarto Pub Limitations: Quarto Pub has some limitations, especially for large or complex websites. For more advanced hosting options, consider using other platforms like Netlify or GitHub Pages.\nFree tier limitations: The free tier of Quarto Pub has some limitations, regarding the number of deployments, and storage. Refer to the Quarto Pub documentation for the most up to date limitations."
  },
  {
    "objectID": "sessions/week1/1-intro.html#further-exploration-1",
    "href": "sessions/week1/1-intro.html#further-exploration-1",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Further Exploration",
    "text": "Further Exploration\n\nExplore the Quarto Pub documentation for more advanced features: quarto.pub.\nLearn about Quarto projects for creating multi-page websites.\nExperiment with different Quarto Pub settings and options."
  },
  {
    "objectID": "sessions/week1/1-intro.html#exercise-1",
    "href": "sessions/week1/1-intro.html#exercise-1",
    "title": "Week 1 - Introduction to RMDA",
    "section": "Exercise",
    "text": "Exercise\nNow create your own quarto workbook:\n1 - Download the zip file of the project\n2- Tweak the *qmd file as you wish\n3 - Publish your workbook\n4 - Submit to the link to NOW dropbox folder"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html",
    "href": "sessions/week3/3-data_expl.html",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#learning-objectives",
    "href": "sessions/week3/3-data_expl.html#learning-objectives",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\n\nResearch Methods\nData Analyses\n\n\n\n\nDistinguish between scientific and statistical hypotheses\nProduce informative summaries of data\n\n\nElaborate sounding scientific hypotheses\nUnderstand moments of distribution\n\n\n\nBasics of data visualization"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#prerequisites",
    "href": "sessions/week3/3-data_expl.html#prerequisites",
    "title": "Week 3 - Data Exploration & Scientific Hypotheses",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you begin, ensure you have the following installed:\n\nR: You can download the latest version from the official R website. Instructions for different operating systems (Windows, Mac OS, UNIX/Linux) are available [8]. R must be installed before installing RStudio [9].\nRStudio: Download and install RStudio Desktop from https://www.rstudio.com/ [10]. To open RStudio, locate it in your applications and launch it [11].\ntidyverse package: Install the tidyverse package within R. Open RStudio and in the Console pane, type and run the following command [12]:\ninstall.packages(\"tidyverse\")\n\nThis command needs to be run only the first time you want to use the tidyverse. You can also install packages by navigating to Tools -&gt; Install Packages in RStudio."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#readings",
    "href": "sessions/week3/3-data_expl.html#readings",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Readings",
    "text": "Readings\nFor Data Analyses\n\n Check chapter 4 of the e-book Tidyverse Skills for Data Science\n Data visualization with ggplot2 :: Cheat Sheet\n Data summaries with dplyr\n\nFor Research Methods\n\n The hypotheses in science writing\n Formulating Hypotheses for Different Study Designs"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#learning-objectives-1",
    "href": "sessions/week3/3-data_expl.html#learning-objectives-1",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDemonstrate how to subset, merge, and create new datasets from existing data structures in R.\nPerform basic data wrangling with functions in the Tidyverse package."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#tidyverse-basics",
    "href": "sessions/week3/3-data_expl.html#tidyverse-basics",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "Tidyverse basics",
    "text": "Tidyverse basics\nThe Tidyverse suite of packages introduces users to a set of data structures, functions and operators to make working with data more intuitive, but is slightly different from the way we do things in base R. Two important new concepts we will focus on are pipes and tibbles.\nBefore we get started with pipes or tibbles, let’s load the library:\nlibrary(tidyverse)\n\nPipes\nStringing together commands in R can be quite daunting. Also, trying to understand code that has many nested functions can be confusing.\nTo make R code more human readable, the Tidyverse tools use the pipe, %&gt;%, which was acquired from the magrittr package and is now part of the dplyr package that is installed automatically with Tidyverse. The pipe allows the output of a previous command to be used as input to another command instead of using nested functions.\n\nNOTE: Shortcut to write the pipe is shift + command/ctrl + M\n\nAn example of using the pipe to run multiple commands:\n## A single command\nsqrt(83)\n\n## Base R method of running more than one command\nround(sqrt(83), digits = 2)\n\n## Running more than one command with piping\nsqrt(83) %&gt;% round(digits = 2)\nThe pipe represents a much easier way of writing and deciphering R code, and so we will be taking advantage of it, when possible, as we work through the remaining lesson.\n\nExercises\n\nCreate a vector of random numbers using the code below:\nrandom_numbers &lt;- c(81, 90, 65, 43, 71, 29)\nUse the pipe (%&gt;%) to perform two steps in a single line:\n\nTake the mean of random_numbers using the mean() function.\nRound the output to three digits using the round() function.\n\n\n\n\n\nTibbles\nA core component of the tidyverse is the tibble. Tibbles are a modern rework of the standard data.frame, with some internal improvements to make code more reliable. They are data frames, but do not follow all of the same rules. For example, tibbles can have numbers/symbols for column names, which is not normally allowed in base R.\nImportant: tidyverse is very opininated about row names. These packages insist that all column data (e.g. data.frame) be treated equally, and that special designation of a column as rownames should be deprecated. Tibble provides simple utility functions to handle rownames: rownames_to_column() and column_to_rownames().\nTibbles can be created directly using the tibble() function or data frames can be converted into tibbles using as_tibble(name_of_df).\n\nNOTE: The function as_tibble() will ignore row names, so if a column representing the row names is needed, then the function rownames_to_column(name_of_df) should be run prior to turning the data.frame into a tibble. Also, as_tibble() will not coerce character vectors to factors by default."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#experimental-data",
    "href": "sessions/week3/3-data_expl.html#experimental-data",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "Experimental data",
    "text": "Experimental data\nWe’re going to explore the Tidyverse suite of tools to wrangle our data to prepare it for visualization. You should have downloaded the file called gprofiler_results_Mov10oe.tsv into your R project’s data folder earlier.\n\nIf you do not have the gprofiler_results_Mov10oe.tsv file in your data folder, you can right click and download it into the data folder using this link.\n\nThe dataset:\n\nRepresents the functional analysis results, including the biological processes, functions, pathways, or conditions that are over-represented in a given list of genes.\nOur gene list was generated by differential gene expression analysis and the genes represent differences between control mice and mice over-expressing a gene involved in RNA splicing.\n\nThe functional analysis that we will focus on involves gene ontology (GO) terms, which:\n\ndescribe the roles of genes and gene products\norganized into three controlled vocabularies/ontologies (domains):\n\nbiological processes (BP)\ncellular components (CC)\nmolecular functions (MF)"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#analysis-goal-and-workflow",
    "href": "sessions/week3/3-data_expl.html#analysis-goal-and-workflow",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "Analysis goal and workflow",
    "text": "Analysis goal and workflow\nGoal: Visually compare the most significant biological processes (BP) based on the number of associated differentially expressed genes (gene ratios) and significance values by creating the following plot:\n\n\n\ndotplot6\n\n\nTo wrangle our data in preparation for the plotting, we are going to use the Tidyverse suite of tools to wrangle and visualize our data through several steps:\n\nRead in the functional analysis results\nExtract only the GO biological processes (BP) of interest\nSelect only the columns needed for visualization\nOrder by significance (p-adjusted values)\nRename columns to be more intuitive\nCreate additional metrics for plotting (e.g. gene ratios)\nPlot results"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#tidyverse-tools",
    "href": "sessions/week3/3-data_expl.html#tidyverse-tools",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "Tidyverse tools",
    "text": "Tidyverse tools\nWhile all of the tools in the Tidyverse suite are deserving of being explored in more depth, we are going to investigate more deeply the reading (readr), wrangling (dplyr), and plotting (ggplot2) tools."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#read-in-the-functional-analysis-results",
    "href": "sessions/week3/3-data_expl.html#read-in-the-functional-analysis-results",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "1. Read in the functional analysis results",
    "text": "1. Read in the functional analysis results\nWhile the base R packages have perfectly fine methods for reading in data, the readr and readxl Tidyverse packages offer additional methods for reading in data. Let’s read in our tab-delimited functional analysis results using read_delim():\n# Read in the functional analysis results\nfunctional_GO_results &lt;- read_delim(file = \"data/gprofiler_results_Mov10oe.tsv\", delim = \"\\t\" )\n\n# Take a look at the results\nfunctional_GO_results\n\n\nClick here to see how to do this in base R\n\n\n# Read in the functional analysis results\nfunctional_GO_results &lt;- read.delim(file = \"data/gprofiler_results_Mov10oe.tsv\", sep = \"\\t\" )\n# Take a look at the results\nfunctional_GO_results\n\n\nNotice that the results were automatically read in as a tibble and the output gives the number of rows, columns and the data type for each of the columns.\n\nNOTE: A large number of tidyverse functions will work with both tibbles and dataframes, and the data structure of the output will be identical to the input. However, there are some functions that will return a tibble (without row names), whether or not a tibble or dataframe is provided."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#extract-only-the-go-biological-processes-bp-of-interest",
    "href": "sessions/week3/3-data_expl.html#extract-only-the-go-biological-processes-bp-of-interest",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "2. Extract only the GO biological processes (BP) of interest",
    "text": "2. Extract only the GO biological processes (BP) of interest\nNow that we have our data, we will need to wrangle it into a format ready for plotting. For all of our data wrangling steps we will be using tools from the dplyr package, which is a swiss-army knife for data wrangling of data frames.\nTo extract the biological processes of interest, we only want those rows where the domain is equal to BP, which we can do using the filter() function.\nTo filter rows of a data frame/tibble based on values in different columns, we give a logical expression as input to the filter() function to return those rows for which the expression is TRUE.\nNow let’s return only those rows that have a domain of BP:\n# Return only GO biological processes\nbp_oe &lt;- functional_GO_results %&gt;%\n  filter(domain == \"BP\")\n  \nView(bp_oe)\n\n\nClick here to see how to do this in base R\n\n\n# Return only GO biological processes\nidx &lt;- functional_GO_results$domain == \"BP\"\nbp_oe2 &lt;- functional_GO_results[idx,]\nView(bp_oe)\n\n\nNow we have returned only those rows with a domain of BP. How have the dimensions of our results changed?\n\nExercise:\nWe would like to perform an additional round of filtering to only keep the most specific GO terms.\n\nFor bp_oe, use the filter() function to only keep those rows where the relative.depth is greater than 4.\nSave output to overwrite our bp_oe variable."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#select-only-the-columns-needed-for-visualization",
    "href": "sessions/week3/3-data_expl.html#select-only-the-columns-needed-for-visualization",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "3. Select only the columns needed for visualization",
    "text": "3. Select only the columns needed for visualization\nFor visualization purposes, we are only interested in the columns related to the GO terms, the significance of the terms, and information about the number of genes associated with the terms.\nTo extract columns from a data frame/tibble we can use the select() function. In contrast to base R, we do not need to put the column names in quotes for selection.\n# Selecting columns to keep\nbp_oe &lt;- bp_oe %&gt;%\n  select(term.id, term.name, p.value, query.size, term.size, overlap.size, intersection)\n\nView(bp_oe)\n\n\nClick here to see how to do this in base R\n\n\n# Selecting columns to keep\nbp_oe &lt;- bp_oe[, c(\"term.id\", \"term.name\", \"p.value\", \"query.size\", \"term.size\", \"overlap.size\", \"intersection\")]\nView(bp_oe)\n\n\nThe select() function also allows for negative selection. So we could have alternately removed columns with negative selection. Note that we need to put the column names inside of the combine (c()) function with a - preceding it for this functionality.\n# DO NOT RUN\n# Selecting columns to remove\nbp_oe &lt;- bp_oe %&gt;%\n    select(-c(query.number, significant, recall, precision, subgraph.number, relative.depth, domain))\n\n\nClick here to see how to do this in base R\n\n\n# DO NOT RUN\n# Selecting columns to remove\nidx &lt;- !(colnames(bp_oe) %in% c(\"query.number\", \"significant\", \"recall\", \"precision\", \"subgraph.number\", \"relative.depth\", \"domain\"))\nbp_oe &lt;- bp_oe[, idx]"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#order-go-processes-by-significance-adjusted-p-values",
    "href": "sessions/week3/3-data_expl.html#order-go-processes-by-significance-adjusted-p-values",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "4. Order GO processes by significance (adjusted p-values)",
    "text": "4. Order GO processes by significance (adjusted p-values)\nNow that we have only the rows and columns of interest, let’s arrange these by significance, which is denoted by the adjusted p-value.\nLet’s sort the rows by adjusted p-value with the arrange() function.\n# Order by adjusted p-value ascending\nbp_oe &lt;- bp_oe %&gt;%\n  arrange(p.value)\n\n\nClick here to see how to do this in base R\n\n\n# Order by adjusted p-value ascending\nidx &lt;- order(bp_oe$p.value)\nbp_oe &lt;- bp_oe[idx,]\n\n\n\nNOTE1: If you wanted to arrange in descending order, then you could have run the following instead:\n# DO NOT RUN\n# Order by adjusted p-value descending\nbp_oe &lt;- bp_oe %&gt;%\n  arrange(desc(p.value))\n\n\nClick here to see how to do this in base R\n\n\n# DO NOT RUN\n# Order by adjusted p-value descending\nidx &lt;- order(bp_oe$p.value, decreasing = TRUE)\nbp_oe &lt;- bp_oe[idx,]\n\n\n\nNOTE2: Ordering variables in ggplot2 is a bit different. This post introduces a few ways of ordering variables in a plot."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#rename-columns-to-be-more-intuitive",
    "href": "sessions/week3/3-data_expl.html#rename-columns-to-be-more-intuitive",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "5. Rename columns to be more intuitive",
    "text": "5. Rename columns to be more intuitive\nWhile not necessary for our visualization, renaming columns more intuitively can help with our understanding of the data using the rename() function. The syntax is new_name = old_name.\nLet’s rename the term.id and term.name columns.\n# Provide better names for columns\nbp_oe &lt;- bp_oe %&gt;% \n  dplyr::rename(GO_id = term.id, \n                GO_term = term.name)\n\n\nClick here to see how to do this in base R\n\n\n# Provide better names for columns\ncolnames(bp_oe)[colnames(bp_oe) == \"term.id\"] &lt;- \"GO_id\"\ncolnames(bp_oe)[colnames(bp_oe) == \"term.name\"] &lt;- \"GO_term\"\n\n\n\nNOTE: In the case of two packages with identical function names, you can use :: with the package name before and the function name after (e.g stats::filter()) to ensure that the correct function is implemented. The :: can also be used to bring in a function from a library without loading it first.\nIn the example above, we wanted to use the rename() function specifically from the dplyr package, and not any of the other packages (or base R) which may have the rename() function.\n\n\nExercise\nRename the intersection column to genes to reflect the fact that these are the DE genes associated with the GO process."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "href": "sessions/week3/3-data_expl.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "6. Create additional metrics for plotting (e.g. gene ratios)",
    "text": "6. Create additional metrics for plotting (e.g. gene ratios)\nFinally, before we plot our data, we need to create a couple of additional metrics. The mutate() function enables you to create a new column from an existing column.\nLet’s generate gene ratios to reflect the number of DE genes associated with each GO process relative to the total number of DE genes.\n# Create gene ratio column based on other columns in dataset\nbp_oe &lt;- bp_oe %&gt;%\n  mutate(gene_ratio = overlap.size / query.size)\n\n\nClick here to see how to do this in base R\n\n\n# Create gene ratio column based on other columns in dataset\nbp_oe &lt;- cbind(bp_oe, gene_ratio = bp_oe$overlap.size / bp_oe$query.size)\n\n\n\nExercise\nCreate a column in bp_oe called term_percent to determine the percent of DE genes associated with the GO term relative to the total number of genes associated with the GO term (overlap.size / term.size)\n\nOur final data for plotting should look like the table below:"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#next-steps",
    "href": "sessions/week3/3-data_expl.html#next-steps",
    "title": "Week 4 Data Exploration & Scientific Hypotheses",
    "section": "Next steps",
    "text": "Next steps\nNow that we have our results ready for plotting, we can use the ggplot2 package to plot our results. If you are interested, you can follow this lesson and dive into how to use ggplot2 to create the plots with this dataset.\n\nAdditional resources\n\nR for Data Science\nteach the tidyverse\ntidy style guide\n\n\nThis lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#research-methods",
    "href": "sessions/week3/3-data_expl.html#research-methods",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Research Methods",
    "text": "Research Methods"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#what-is-a-scientific-hypothesis",
    "href": "sessions/week3/3-data_expl.html#what-is-a-scientific-hypothesis",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "What is a scientific hypothesis?",
    "text": "What is a scientific hypothesis?\n    View slides in full screen\n       \n      \n    \n  \nAccording to the Britannica, a scientific hypothesis is: “… an idea that proposes a tentative explanation about a phenomenon or a narrow set of phenomena observed in the natural world.”\n\nDeductive reasoning and inductive reasoning are two inseparable but distinct processes within scientific research. They reasonate with each other in a cyclical manner to advance scientific knowledge.\nHere’s how they connect:\nInductive reasoning often initiates the scientific process. It begins with empirical observations of the real world. By noticing patterns in these observations, scientists use inductive reasoning to construct broad generalisations or theories. For example, observing that apples, bananas, and oranges grow on trees might lead to the inductive generalisation that all fruit grows on trees.\nOnce a theory is formulated through inductive reasoning, it then serves as a basis for deductive reasoning. Deductive reasoning starts with a generalisation or hypothesis (derived from the theory) and uses it to reach logical conclusions about the real world. If the hypothesis is correct, then the logical conclusions reached through deductive reasoning should also be correct.\nFor instance, if the theory is that all living things require energy to survive, then deductive reasoning would lead to the conclusion that ducks, being living things, require energy to survive.\nScientists use deductive reasoning to empirically test the hypotheses that are generated from their inductively developed theories. They design studies and experiments to see if their logical conclusions hold true in the real world.\nThe results of these deductive tests then feed back into the scientific process. If the results are consistent with the theory (and thus the hypothesis), the theory is supported. However, if the results are not consistent, the theory may need to be modified and refined, leading to the generation of new hypotheses that will again be tested deductively. This creates a circular process where observations lead to theories (induction), theories lead to testable predictions (hypotheses), and those predictions are tested against further observations (deduction), which in turn can refine the theories. In essence, induction is often about building up from specific observations to broader ideas, while deduction is about breaking down broader ideas into specific, testable predictions. They work together, with inductive reasoning often paving the way for deductive testing, and the outcomes of deductive testing influencing the further development of theories arrived at through induction. Some research approaches, like case studies, lean more heavily on inductive processes, while experimental research often emphasises deductive reasoning.\n\n\n\n\n\n\nTip\n\n\n\nScientists use inductive reasoning to formulate theories, which then lead to hypotheses that are tested using deductive reasoning. In essence, science involves both inductive and deductive processes. Research approaches like case studies, which heavily rely on empirical observations and gathering large amounts of data to find interesting patterns and new ideas, are closely associated with inductive processes."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#research-vs-statistical-hypotheses",
    "href": "sessions/week3/3-data_expl.html#research-vs-statistical-hypotheses",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Research vs Statistical Hypotheses",
    "text": "Research vs Statistical Hypotheses\n\nA scientific hypothesis is a proposed explanation for an observation, phenomenon, or scientific problem. It must be based on observations and make a testable and reproducible prediction about reality. A scientific hypothesis is a provisional idea whose merit requires evaluation and requires further work to either confirm or disprove it. If repeatedly demonstrated to be true by experiment, a scientific hypothesis may become part of a scientific theory. In its essence, a scientific hypothesis aims to be true, adequate, accurate or useful in explaining some natural phenomenon. It guides the types of data we collect and the analyses we conduct.\nA statistical hypothesis, on the other hand, is specifically used when investigating a possible correlation or similar relation between phenomena. In such cases, the hypothesis that a relation exists is not examined in the same way as a proposed new law of nature. Instead, statistical tests are employed to determine how likely it is that the observed overall effect would occur if the hypothesised relation does not actually exist.\nSome key distinctions are:\n\nScope and Generality: A scientific hypothesis is often a broader proposed explanation, while a statistical hypothesis is a more specific statement about the relationship between variables that is subjected to statistical testing. For instance, a scientific hypothesis might be that “sunlight is necessary for seeds to grow”. A related statistical hypothesis could be that “seeds grown in bags wrapped in aluminium foil will produce shorter plants on average compared to seeds grown in bags not wrapped in foil”.\nMethod of Evaluation: Scientific hypotheses are evaluated through the broader scientific process, which involves observation, experimentation, and analysis. This can include various research approaches. Statistical hypotheses are specifically evaluated using statistical tests. These tests involve comparing a null hypothesis (typically stating no relation) with an alternative hypothesis (stating a relation exists) and determining the likelihood of the observed data under the null hypothesis. A decision is then made based on a pre-determined significance level.\nLevel of Abstraction: Scientific hypotheses often deal with underlying mechanisms or causes of phenomena. Statistical hypotheses are more directly concerned with patterns and relationships within data. The PMC source discusses the TASI model, where statistical assumptions are considered necessary to test an empirical hypothesis which is derived from a theoretical hypothesis using auxiliary assumptions. This highlights that statistical hypotheses are often a step in testing broader scientific ideas.\nForm of Statement: While scientific hypotheses can be verbal or formal (e.g., mathematical models), statistical hypotheses are often formulated in terms of population parameters and distributions, making them amenable to statistical analysis.\n\n\n\n\n\n\n\nNote\n\n\n\nIn summary, a scientific hypothesis proposes an explanation for a phenomenon and guides the research process, while a statistical hypothesis is a specific, testable statement about data relationships derived from a scientific hypothesis, which is evaluated using statistical methods. The statistical hypothesis provides a way to quantitatively assess some aspect of the broader scientific hypothesis."
  },
  {
    "objectID": "example_presentation.html",
    "href": "example_presentation.html",
    "title": "Quiz example",
    "section": "",
    "text": "Bill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google"
  },
  {
    "objectID": "example_presentation.html#a-basic-example",
    "href": "example_presentation.html#a-basic-example",
    "title": "Quiz example",
    "section": "",
    "text": "Bill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google"
  },
  {
    "objectID": "example_presentation.html#a-quiz-with-a-clear-answer-button",
    "href": "example_presentation.html#a-quiz-with-a-clear-answer-button",
    "title": "Quiz example",
    "section": "A quiz with a clear answer button",
    "text": "A quiz with a clear answer button\n\nBill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google\n\n\nClear answer"
  },
  {
    "objectID": "example_presentation.html#a-quiz-with-additional-buttons",
    "href": "example_presentation.html#a-quiz-with-additional-buttons",
    "title": "Quiz example",
    "section": "A quiz with additional buttons",
    "text": "A quiz with additional buttons\n\nBill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google\n\n\nClear answer\n\n\n\n\n\n\n\n\n\n\nShow hint\n\nThe company name starts with an ‘M’…\n\n\n\n\n\n\nShow Answer\n\nBill Gates and Paul Allen founded Microsoft on April 4, 1975."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Quiz example",
    "section": "",
    "text": "This document illustrates the usage of the naquiz quarto extension.\nThe extension enables adding multiple choice questions when using HTML documents. It also adds Alpine.js javascript framework to the document."
  },
  {
    "objectID": "example.html#a-basic-example",
    "href": "example.html#a-basic-example",
    "title": "Quiz example",
    "section": "A basic example",
    "text": "A basic example\n\nBill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google"
  },
  {
    "objectID": "example.html#a-quiz-with-a-clear-answer-button",
    "href": "example.html#a-quiz-with-a-clear-answer-button",
    "title": "Quiz example",
    "section": "A quiz with a clear answer button",
    "text": "A quiz with a clear answer button\n\nBill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google\n\n\nClear answer"
  },
  {
    "objectID": "example.html#a-quiz-with-additional-buttons",
    "href": "example.html#a-quiz-with-additional-buttons",
    "title": "Quiz example",
    "section": "A quiz with additional buttons",
    "text": "A quiz with additional buttons\n\nBill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google\n\n\nClear answer\n\n\n\n\n\n\n\n\n\n\n\nShow hint\n\nThe company name starts with an ‘M’…\n\n\n\n\n\n\nShow Answer\n\nBill Gates and Paul Allen founded Microsoft on April 4, 1975."
  },
  {
    "objectID": "example.html#a-quiz-with-additional-information-in-callouts",
    "href": "example.html#a-quiz-with-additional-information-in-callouts",
    "title": "Quiz example",
    "section": "A quiz with additional information in callouts",
    "text": "A quiz with additional information in callouts\n\nBill Gates was the founder of:\n\n\n\n\n ✗Apple\n\n\n ✓Microsoft\n\n\n ✗Facebook\n\n\n ✗Google\n\n\nClear answer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe company name starts with an ‘M’…\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBill Gates and Paul Allen founded Microsoft on April 4, 1975."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#what-is-a-scientific-hypothesis-1",
    "href": "sessions/week3/3-data_expl.html#what-is-a-scientific-hypothesis-1",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "What is a scientific hypothesis?",
    "text": "What is a scientific hypothesis?"
  },
  {
    "objectID": "assessment.html",
    "href": "assessment.html",
    "title": "Assessment Brief 2025-26",
    "section": "",
    "text": "This Module covers the following courses:\n\nMSc/MRes Applied Ecology & Geospatial Techniques\nMSc/MRes Biodiversity Conservation\nMSc/MRes Endangered Species Recovery & Conservation\nMSc/MRes Equine Performance, Health & Welfare MSc/MRes Smart Agriculture"
  },
  {
    "objectID": "assessment.html#summative-assessment-brief",
    "href": "assessment.html#summative-assessment-brief",
    "title": "Assessment Brief 2025-26",
    "section": "",
    "text": "This Module covers the following courses:\n\nMSc/MRes Applied Ecology & Geospatial Techniques\nMSc/MRes Biodiversity Conservation\nMSc/MRes Endangered Species Recovery & Conservation\nMSc/MRes Equine Performance, Health & Welfare MSc/MRes Smart Agriculture"
  },
  {
    "objectID": "assessment.html#formative-assessments",
    "href": "assessment.html#formative-assessments",
    "title": "Assessment Brief 2025-26",
    "section": "Formative Assessments",
    "text": "Formative Assessments\nYour formative opportunities are related to the development of your workbook and sharing of this development with module leader and colleagues.\n\n\n\n\n\n\nShare your Workbook\n\n\n\nYou must not be shy or embarrassed to share your workbook. Nobody will judge you. We are all learning!"
  },
  {
    "objectID": "assessment.html#type-of-assessment",
    "href": "assessment.html#type-of-assessment",
    "title": "Assessment Brief 2025-26",
    "section": "Type of Assessment",
    "text": "Type of Assessment\nYour Summative is to produce an “article-like” dynamic document containing an analytical workflow for a given dataset where critically interpret the results and draw your conclusions.\n\nStep 1 - Context\nYou will be given a context of a scientific problem. This context will introduce you to a real-world problem that is going to be used as a background for the data that you are going to analyse.\n\n\nStep 2 - The dataset\nYou will have access to a pre-built dataset in a .csv format that will contain all the data and metadata needed for your analyses. The data is the same for everyone and I am not providing data on your specific subject of studies.\n\n\nStep 3 - Your Workbook\nYou are going to work on the assessment using your Quarto workbook constructed over the course of the whole module. This is going to be your submission. I’ll collect your exams using a NOW Dropbox where you can paste the link to your published workbook.\n\n\n\n\n\n\nNote\n\n\n\nThe dataset and context description for the Summative exam will be shared by the end of the module sessions (probably in the last week of classes)."
  },
  {
    "objectID": "assessment.html#transferable-skills-developed-in-this-assessment",
    "href": "assessment.html#transferable-skills-developed-in-this-assessment",
    "title": "Assessment Brief 2025-26",
    "section": "Transferable skills developed in this assessment",
    "text": "Transferable skills developed in this assessment\n\nAnalytical workflow\n\nCreate and attach to an analytical workflow that is reproducible\nGenerate good quality graphs and tables\nComment and understand R code\nIdentify core results within a set of exploratory analyses.\nInterpret and generate conclusions based on data analysed"
  },
  {
    "objectID": "assessment.html#specific-tasks",
    "href": "assessment.html#specific-tasks",
    "title": "Assessment Brief 2025-26",
    "section": "Specific tasks",
    "text": "Specific tasks\n\nDesign an experimental design\nDescribe methods for reproducibility\nCreate a analytical workflow\nComment on the R codes in your workbook"
  },
  {
    "objectID": "assessment.html#assessment-guidance",
    "href": "assessment.html#assessment-guidance",
    "title": "Assessment Brief 2025-26",
    "section": "Assessment Guidance",
    "text": "Assessment Guidance\n\nAnalytical workflow (15%)\nR code commented (15%)\nExploratory analysis (20%)\nQuality graphs and tables (25%)\nInterpretation of results (25%)"
  },
  {
    "objectID": "assessment.html#further-information",
    "href": "assessment.html#further-information",
    "title": "Assessment Brief 2025-26",
    "section": "Further information",
    "text": "Further information\n\nExtenuating circumstances"
  },
  {
    "objectID": "assessment.html#grading-matrix",
    "href": "assessment.html#grading-matrix",
    "title": "Assessment Brief 2025-26",
    "section": "Grading Matrix",
    "text": "Grading Matrix\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nFail\nLow | Mid\nMarginal Fail\nPass\nLow | Mid | High\nCommendation\nLow | Mid | High\nDistinction\nLow | Mid |High\nDistinction\nExceptional\n\n\n\n\nAnalytical workflow\nNo clear analytical workflow\nWorkflow not easy to find; mixed analytical approach in search for any significant p-value\nWorkflow relatively reasonable but with excess of flaw analyses and lack of a logic sequence that goes from 1) preparation; 2) data wrangling; 3) Exploratory analyses; 4) Core analyses\nClear workflow but hard to reproduce because crucial steps were either omitted or non commented.\nVery good workflow with clear guidance for reproducibility\nHigh quality workflow, fully reproducible and extensively commented\n\n\nR code commented\nLittle to no comments on coding\nComments provided but non meaningful for crucial steps\nCodes mostly commented but crucial steps are not udnerstood\nCodes mostly commented and helping reproducibility.\nCodes fully commented but not excessively, avoiding visual pollution\nCodes fully commented and not affecting visual inspection of the script and allows full reproducibility and explanation in key steps\n\n\nExploratory analysis\nNo exploratory analyses done\nInsufficient exploratory analyses\nEnough exploratory analyses but not commented or justified\nGood exploratory analyses but poorly commented and little justified\nVery good exploratory analyses, commented and justified\nWorld-class and fully justified exploratory approach to data\n\n\nQuality graphs and tables\nPoor graphs, lacking crucial elements such as axis title and captions\nPoor graphs and tables with some elements present but poorly explained while other elements are missig\nGraphs and tables present with most elements, but some missing components preclude full understanding of the info presented\nGraphs and tables of acceptable quality with all elements present but not clearly descripted\nGood quality graphs and tables that could be accepted for publication in any serious scientific journal\nOutstading graphs and tables with graphical abstracts and schematic figures. All elements present and fully explained.\n\n\nInterpretation of results\nPoor or inexistant critical interpretation of the results found\nDeficient interpretation of the results and misuse of statistical concepts and wrong translation of tests and graphs\nResults are just reported with no critical interpretation or further discussion\nResults correctly reported and critically interpreted but excessive speculation is present\nResults are fully reported in a correct manner with string attachment to the proposed workflow and are discussed without much speculation\nExcellent interpretation, creative and fully connected with scientific hypotheses"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#part-1",
    "href": "sessions/week3/3-data_expl.html#part-1",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Part 1",
    "text": "Part 1"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#part-2",
    "href": "sessions/week3/3-data_expl.html#part-2",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Part 2",
    "text": "Part 2\n    View slides in full screen"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#data-summarisation-with-dplyr",
    "href": "sessions/week3/3-data_expl.html#data-summarisation-with-dplyr",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Data summarisation with dplyr",
    "text": "Data summarisation with dplyr\nData summarisation is a crucial step in understanding your data. It involves calculating descriptive statistics and creating summary tables to get an overview of the dataset, identify patterns, and spot potential issues. The dplyr package within the tidyverse provides a set of intuitive functions for efficient data manipulation, including powerful tools for summarisation. The tidyverse expands the vocabulary of R.\nTo begin, we first need to load the tidyverse package, which includes dplyr:\n\n# Load the tidyverse package [5, 6]\nlibrary(tidyverse)"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#summary-statistics-for-ungrouped-data",
    "href": "sessions/week3/3-data_expl.html#summary-statistics-for-ungrouped-data",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Summary Statistics for Ungrouped Data",
    "text": "Summary Statistics for Ungrouped Data\nThe summarise() function in dplyr is used to compute summary statistics for your data1 …. When applied to an ungrouped data frame, it calculates the specified summaries across all rows1 .\nLet’s use the built-in iris dataset as an example :\n\n# Convert the iris dataset to a tibble\nmy_data &lt;- as_tibble(iris)\nmy_data\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\nWe can compute the mean of Sepal.Length and Petal.Length, as well as the total number of observations using summarise() and the n() function1:\n\n# Calculate summary statistics for the entire dataset\nmy_data %&gt;%\n  summarise(\n    count = n(), # Count the number of rows [7, 8]\n    mean_sep_length = mean(Sepal.Length, na.rm = TRUE), # Calculate the mean of Sepal.Length, removing NA values [6]\n    mean_pet_length = mean(Petal.Length, na.rm = TRUE)  # Calculate the mean of Petal.Length, removing NA values [6]\n  )\n\n# A tibble: 1 × 3\n  count mean_sep_length mean_pet_length\n  &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1   150            5.84            3.76\n\n\nThe na.rm = TRUE argument is used to handle missing values by removing them before computing the mean5 . R is clear about trying to do calculations when there is an NA5 . If there is an NA, it cannot create a correct calculation, so it will return NA again5 . This is a nice way of quickly seeing that you have missing values in your data5 ## Summary Statistics for Grouped Data\nOften, we want to compute summary statistics for different groups within our data. This is This is achieved by using the group_by() function before summarise() group_by() takes one or more column names as arguments and groups the data based on the unique values in those columns. Subsequent summarise() operations will then be performed within each group.\nLet’s group the iris data by Species and calculate the same summary statistics as before:\n\n# Group the data by Species and then summarise [1, 9]\nmy_data %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    count = n(), # Count the number of rows in each group [7, 8]\n    mean_sep_length = mean(Sepal.Length), # Calculate the mean of Sepal.Length for each species [1]\n    mean_pet_length = mean(Petal.Length)  # Calculate the mean of Petal.Length for each species [1]\n  )\n\n# A tibble: 3 × 4\n  Species    count mean_sep_length mean_pet_length\n  &lt;fct&gt;      &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1 setosa        50            5.01            1.46\n2 versicolor    50            5.94            4.26\n3 virginica     50            6.59            5.55\n\n\nWe can also group by multiple variables: Let’s use the ToothGrowth dataset:\n\n# Display the head of the ToothGrowth dataset\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n\nNow, let’s group by supp and dose and calculate the number of observations, mean, and standard deviation of len:\n\n# Group by two variables and calculate summaries [1]\nToothGrowth %&gt;%\n  group_by(supp, dose) %&gt;%\n  summarise(\n    n = n(), # Count observations in each group [7]\n    mean_length = mean(len), # Calculate the mean of len [10]\n    sd_length = sd(len)      # Calculate the standard deviation of len [10]\n  )\n\n`summarise()` has grouped output by 'supp'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 5\n# Groups:   supp [2]\n  supp   dose     n mean_length sd_length\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 OJ      0.5    10       13.2       4.46\n2 OJ      1      10       22.7       3.91\n3 OJ      2      10       26.1       2.66\n4 VC      0.5    10        7.98      2.75\n5 VC      1      10       16.8       2.52\n6 VC      2      10       26.1       4.80"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#summarising-multiple-variables",
    "href": "sessions/week3/3-data_expl.html#summarising-multiple-variables",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Summarising Multiple Variables",
    "text": "Summarising Multiple Variables\ndplyr provides several functions to summarise multiple variables efficiently:\n• summarise_all(): Applies a summary function to every column in the data frame.\n• summarise_at(): Applies summary functions to specific columns selected using a character vector.\n• summarise_if(): Applies summary functions to columns selected with a predicate function that returns TRUE. For example, we can summarise only numeric columns:\nThe simplified formats for these functions are:\n• summarise_all(.tbl, .funs, ...)\n• summarise_if(.tbl, .predicate, .funs, ...)\n• summarise_at(.tbl, .vars, .funs, ...)\nWhere .funs can be a function name or a list of function calls, and … allows for additional arguments to the functions (e.g., na.rm = TRUE)."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#useful-statistical-summary-functions",
    "href": "sessions/week3/3-data_expl.html#useful-statistical-summary-functions",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Useful Statistical Summary Functions",
    "text": "Useful Statistical Summary Functions\ndplyr works well with various R functions that compute statistical summaries:\n• Measures of centrality: mean(),median();\n• Measures of variation: sd() (standard deviation), IQR() (interquartile range) , range()(range of the data);\n• Measures of rank: min() (minimum value), max() (maximum value), quantile();\n• Counts: n() (the number of elements), sum(!is.na(x)) (count non-missing values), n_distinct() (count the number of unique values).\n• Counts and proportions of logical values: sum(x &gt; 10) (count the number of elements where x &gt; 10), mean(y == 0) (proportion of elements where y = 0)."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#frequency-tables",
    "href": "sessions/week3/3-data_expl.html#frequency-tables",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Frequency Tables",
    "text": "Frequency Tables\nFor quickly obtaining frequency tables, dplyr provides the count() and tally() functions, which are synonymous.\nUsing the penguins dataset:\n\n# Load the penguins dataset [6]\nlibrary(palmerpenguins)\npenguins &lt;- as_tibble(palmerpenguins::penguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe can get a frequency table of the species column:\n\n# Get a frequency table using tally\npenguins %&gt;%\n  group_by(species) %&gt;%\n  tally()\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nOr equivalently:\n\n# Get a frequency table using count\npenguins %&gt;%\n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nWe can also group by multiple columns to get more detailed frequency tables:\n\n# Get a frequency table with multiple grouping variables\npenguins %&gt;%\n  count(species, island)\n\n# A tibble: 5 × 3\n  species   island        n\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;\n1 Adelie    Biscoe       44\n2 Adelie    Dream        56\n3 Adelie    Torgersen    52\n4 Chinstrap Dream        68\n5 Gentoo    Biscoe      124"
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#adding-summary-statistics-as-new-columns",
    "href": "sessions/week3/3-data_expl.html#adding-summary-statistics-as-new-columns",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Adding Summary Statistics as New Columns",
    "text": "Adding Summary Statistics as New Columns\nWhile summarise() collapses the data frame to summary rows, mutate() can be used in conjunction with group_by() to add summary statistics as new columns to the original data frame.\nFor example, to add the mean bill_length_mm for each island16 :\n\n# Group by island and add the mean bill length as a new column [19]\npenguins %&gt;%\n  drop_na(bill_length_mm) %&gt;% # Remove rows with NA in bill_length_mm\n  group_by(island) %&gt;%\n  mutate(mean_bill_length_island = mean(bill_length_mm)) %&gt;%\n  head()\n\n# A tibble: 6 × 9\n# Groups:   island [1]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           36.7          19.3               193        3450\n5 Adelie  Torgersen           39.3          20.6               190        3650\n6 Adelie  Torgersen           38.9          17.8               181        3625\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, mean_bill_length_island &lt;dbl&gt;\n\n\nRemember to ungroup() the data frame if you want subsequent operations to be performed on the entire dataset rather than by groups."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#conclusion",
    "href": "sessions/week3/3-data_expl.html#conclusion",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Conclusion",
    "text": "Conclusion\nThe dplyr package in the tidyverse offers a powerful and flexible set of tools for data summarisation in R. By combining functions like group_by() and summarise() (along with its variants summarise_all(), summarise_at(), summarise_if()) with various statistical summary functions, you can efficiently generate insightful summaries of your data, both for entire datasets and for specific groups. Functions like count() and tally() provide convenient ways to create frequency tables. These capabilities make dplyr an essential package for data analysis and exploration in R."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#exercise-title-exploring-penguin-data-with-dplyr-summaries",
    "href": "sessions/week3/3-data_expl.html#exercise-title-exploring-penguin-data-with-dplyr-summaries",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Exercise Title: Exploring Penguin Data with dplyr Summaries",
    "text": "Exercise Title: Exploring Penguin Data with dplyr Summaries\nObjective: To practice using the dplyr package in R to calculate and explore summary statistics of the penguins dataset, both for the entire dataset and for specific groups.\nRequired Packages: dplyr and palmerpenguins.\nSteps:\n\nLoad the necessary packages and the penguins dataset:\nCalculate basic summary statistics for the entire dataset:\n\n\nFind the average bill length (in mm) for all penguins, handling missing values.\nCalculate the minimum and maximum flipper length (in mm) for all penguins.\nFind the number of observations in the entire dataset using n().\n\n\nCalculate summary statistics grouped by a single variable (e.g., species):\n\n\nFind the average body mass (in kg) for each penguin species. Remember to convert grams to kilograms by dividing by 1000.\nFor each species, calculate the average and standard deviation of bill depth (in mm).\n\n\nCalculate summary statistics grouped by multiple variables (e.g., species and island):\n\n\nDetermine the number of penguins of each species found on each island using count() or tally().\nCalculate the average flipper length for each combination of species and sex.\n\n\nUse summarise_at() and summarise_if() to summarise multiple columns:\n\n\nCalculate the mean of bill_length_mm and bill_depth_mm for each island using summarise_at().\nCalculate the mean of all numeric columns for each species using summarise_if().\n\n\nExplore frequency tables:\n\n\nCreate a frequency table showing the number of penguins for each species.\nCreate a frequency table showing the number of penguins for each combination of island and sex.\n\n\nAdd summary statistics as a new column using mutate():\n\n\nGroup the data by species and then add a new column showing the average bill length for that species to each row. Remember to ungroup() if needed for subsequent operations on the entire dataset.\n\n\nFurther Exploration (Challenges):\n• Calculate the range (maximum - minimum) of body mass for each species.\n• Find the median flipper length for each island.\n• Determine the proportion of male and female penguins within each species. (Hint: You might need to combine group_by(), count(), and mutate() to calculate proportions).\n• Group the data by island and then find the species with the longest average bill length on each island. (This might involve multiple steps).\nThis exercise will provide practical experience in using dplyr’s powerful functions for data summarisation with the penguins dataset. Remember to refer to the help documentation (e.g., ?summarise) if you need more information on specific functions."
  },
  {
    "objectID": "sessions/week3/3-data_expl.html#exploring-penguin-data-with-dplyr-summaries",
    "href": "sessions/week3/3-data_expl.html#exploring-penguin-data-with-dplyr-summaries",
    "title": "Week 3 - Data Visualisation & Scientific Hypotheses",
    "section": "Exploring Penguin Data with dplyr Summaries",
    "text": "Exploring Penguin Data with dplyr Summaries\nObjective: To practice using the dplyr package in R to calculate and explore summary statistics of the penguins dataset, both for the entire dataset and for specific groups.\nRequired Packages: dplyr and palmerpenguins.\n\n\n\n\n\n\nWarning\n\n\n\nRemember that you must use install.package() fucntion to intall R packages\n\n\nSteps:\n\nLoad the necessary packages and the penguins dataset:\nCalculate basic summary statistics for the entire dataset:\n\n\nFind the average bill length (in mm) for all penguins, handling missing values.\nCalculate the minimum and maximum flipper length (in mm) for all penguins.\nFind the number of observations in the entire dataset using n().\n\n\nCalculate summary statistics grouped by a single variable (e.g., species):\n\n\nFind the average body mass (in kg) for each penguin species. Remember to convert grams to kilograms by dividing by 1000.\nFor each species, calculate the average and standard deviation of bill depth (in mm).\n\n\nCalculate summary statistics grouped by multiple variables (e.g., species and island):\n\n\nDetermine the number of penguins of each species found on each island using count() or tally().\nCalculate the average flipper length for each combination of species and sex.\n\n\nUse summarise_at() and summarise_if() to summarise multiple columns:\n\n\nCalculate the mean of bill_length_mm and bill_depth_mm for each island using summarise_at().\nCalculate the mean of all numeric columns for each species using summarise_if().\n\n\nExplore frequency tables:\n\n\nCreate a frequency table showing the number of penguins for each species.\nCreate a frequency table showing the number of penguins for each combination of island and sex.\n\n\nAdd summary statistics as a new column using mutate():\n\n\nGroup the data by species and then add a new column showing the average bill length for that species to each row. Remember to ungroup() if needed for subsequent operations on the entire dataset.\n\n\nFurther Exploration (Challenges):\n• Calculate the range (maximum - minimum) of body mass for each species.\n• Find the median flipper length for each island.\n• Determine the proportion of male and female penguins within each species. (Hint: You might need to combine group_by(), count(), and mutate() to calculate proportions).\n• Group the data by island and then find the species with the longest average bill length on each island. (This might involve multiple steps).\nThis exercise will provide practical experience in using dplyr’s powerful functions for data summarisation with the penguins dataset. Remember to refer to the help documentation (e.g., ?summarise) if you need more information on specific functions."
  },
  {
    "objectID": "get-started/5-data.html",
    "href": "get-started/5-data.html",
    "title": "Reading in and inspecting data",
    "section": "",
    "text": "Demonstrate how to read existing data into R\nUtilize base R functions to inspect data structures"
  },
  {
    "objectID": "get-started/5-data.html#learning-objectives",
    "href": "get-started/5-data.html#learning-objectives",
    "title": "Reading in and inspecting data",
    "section": "",
    "text": "Demonstrate how to read existing data into R\nUtilize base R functions to inspect data structures"
  },
  {
    "objectID": "get-started/5-data.html#reading-data-into-r",
    "href": "get-started/5-data.html#reading-data-into-r",
    "title": "Reading in and inspecting data",
    "section": "Reading data into R",
    "text": "Reading data into R\n\nThe basics\nRegardless of the specific analysis in R we are performing, we usually need to bring data in for any analysis being done in R, so learning how to read in data is a crucial component of learning to use R.\nMany functions exist to read data in, and the function in R you use will depend on the file format being read in. Below we have a table with some examples of functions that can be used for importing some common text data types (plain text).\n\n\n\n\n\n\n\n\n\nData type\nExtension\nFunction\nPackage\n\n\n\n\nComma separated values\ncsv\nread.csv()\nutils (default)\n\n\n\n\nread_csv()\nreadr (tidyverse)\n\n\nTab separated values\ntsv\nread_tsv()\nreadr\n\n\nOther delimited formats\ntxt\nread.table()\nutils\n\n\n\n\nread_table()\nreadr\n\n\n\n\nread_delim()\nreadr\n\n\n\nFor example, if we have text file where the columns are separated by commas (comma-separated values or comma-delimited), you could use the function read.csv. However, if the data are separated by a different delimiter in a text file (e.g. “:”, “;”, ” “,”), you could use the generic read.table function and specify the delimiter (sep = \" \") as an argument in the function.\n\nNote: The \"\\t\" delimiter is shorthand for tab.\n\nIn the above table we refer to base R functions as being contained in the “utils” package. In addition to base R functions, we have also listed functions from some other packages that can be used to import data, specifically the “readr” package that installs when you install the “tidyverse” suite of packages.\nIn addition to plain text files, you can also import data from other statistical analysis packages and Excel using functions from different packages.\n\n\n\n\n\n\n\n\n\nData type\nExtension\nFunction\nPackage\n\n\n\n\nStata version 13-14\ndta\nreaddta()\nhaven\n\n\nStata version 7-12\ndta\nread.dta()\nforeign\n\n\nSPSS\nsav\nread.spss()\nforeign\n\n\nSAS\nsas7bdat\nread.sas7bdat()\nsas7bdat\n\n\nExcel\nxlsx, xls\nread_excel()\nreadxl (tidyverse)\n\n\n\nNote, that these lists are not comprehensive, and may other functions exist for importing data. Once you have been using R for a bit, maybe you will have a preference for which functions you prefer to use for which data type.\n\n\nMetadata\nWhen working with large datasets, you will very likely be working with “metadata” file which contains the information about each sample in your dataset.\n\nThe metadata is very important information and we encourage you to think about creating a document with as much metadata you can record before you bring the data into R. Here is some additional reading on metadata from the HMS Data Management Working Group.\n\n\nThe read.csv() function\nLet’s bring in the metadata file we downloaded earlier (mouse_exp_design.csv or mouse_exp_design.txt) using the read.csv function.\nFirst, check the arguments for the function using the ? to ensure that you are entering all the information appropriately:\n?read.csv\n\nThe first thing you will notice is that you’ve pulled up the documentation for read.table(), this is because that is the parent function and all the other functions are in the same family.\nThe next item on the documentation page is the function Description, which specifies that the output of this set of functions is going to be a data frame - “Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file.”\nIn usage, all of the arguments listed for read.table() are the default values for all of the family members unless otherwise specified for a given function. Let’s take a look at 2 examples: 1. The separator - * in the case of read.table() it is sep = \"\" (space or tab) * whereas for read.csv() it is sep = \",\" (a comma). 2. The header - This argument refers to the column headers that may (TRUE) or may not (FALSE) exist in the plain text file you are reading in. * in the case of read.table() it is header = FALSE (by default, it assumes you do not have column names) * whereas for read.csv() it is header = TRUE (by default, it assumes that all your columns have names listed).\nThe take-home from the “Usage” section for read.csv() is that it has one mandatory argument, the path to the file and filename in quotations; in our case that is data/mouse_exp_design.csv or data/mouse_exp_design.txt.\n\nThe stringsAsFactors argument\nNote that the read.table {utils} family of functions has an argument called stringsAsFactors, which by default is set to FALSE (you can double check this by searching the Help tab for read.table or running ?read.table in the console).\nIf stringsAsFactors = TRUE, any function in this family of functions will coerce character columns in the data you are reading in to factor columns (i.e., coerce from vector to factor) in the resulting data frame.\nIf you want to maintain the character vector data structure (e.g., for gene names), you will want to make sure that stringsAsFactors = FALSE.\n\n\n\n\nCreate a data frame by reading in the file\nAt this point, please check the extension for the mouse_exp_design file within your data folder. You will have to type it accordingly within the read.csv() function.\n\nread.csv is not fussy about extensions for plain text files, so even though the file we are reading in is a comma-separated value file, it will be read in properly even with a .txt extension.\n\nLet’s read in the mouse_exp_design file and create a new data frame called metadata.\nmetadata &lt;- read.csv(file=\"data/mouse_exp_design.csv\")\n\n# OR \n# metadata &lt;- read.csv(file=\"data/mouse_exp_design.txt\")\n\nNOTE: RStudio supports the automatic completion of code using the Tab key. This is especially helpful for when reading in files to ensure the correct file path. The tab completion feature also provides a shortcut to listing objects, and inline help for functions. Tab completion is your friend! We encourage you to use it whenever possible.\n\n\n\nClick here to see how to import data using the Import Dataset button\n\nYou can also use the Import Dataset button in your Environment pane to import data. This option is not as appealing because it can lack reproducibility if not documented properly, but it can be helpful when getting started. In order to use the Import Dataset:\n\n\nLeft-click the Import Dataset button in the Environment pane\n\n\nLeft-click From Text (base…)\n\n\nNavigate to the file you would like to import and select Open\n\n\nType the name you would like the imported object to be called in the Name textbox.\n\n\nSelect the delimiter that your file is using in the Separator dropdown menu\n\n\nLeft-click Import\n\n\nThese steps are summarized in the GIF below:  Now the dataset has been imported into your environment.\n\nNote: If you are going to use this method, it could impact the reproducibility of your work, because the steps to do that import are not recorded anywhere. If you are going to use this method of importing data, it is STRONGLY RECOMMENDED that you copy the command that read the dataset in and is present in the console to an Rscript file. In the future, you can run that line of code from the Rscript file to recreate the data object.\n\n\n\nGo to your Global environment and click on the name of the data frame you just created.\n\nWhen you do this the metadata table will pop up on the top left hand corner of RStudio, right next to the R script.\n\nYou should see a subtle coloring (blue-gray) of the first row and first column, the rest of the table will have a white background. This is because your first row and first columns have different properties than the rest of the table, they are the names of the rows and columns respectively.\n\nEarlier we noted that the file we just read in had column names (first row of values) and how read.csv() deals with “headers”. In addition to column headers, read.csv() also assumes that the first column contains the row names. Not all functions in the read.table() family of functions will do this and depending on which one you use, you may have to specify an additional argument to properly assign the row names and column names.\n\nRow names and column names are really handy when subsetting data structures and they are also helpful to identify samples or genes. We almost always use them with data frames.\n\n\nExercise 1\n\nDownload this tab-delimited .txt file and save it in your project’s data folder.\nRead it in to R using read.table() with the approriate arguments and store it as the variable proj_summary. To figure out the appropriate arguments to use with read.table(), keep the following in mind:\n\nall the columns in the input text file have column name/headers\nyou want the first column of the text file to be used as row names (hint: look up the input for the row.names = argument in read.table())\n\nDisplay the contents of proj_summary in your console"
  },
  {
    "objectID": "get-started/5-data.html#inspecting-data-structures",
    "href": "get-started/5-data.html#inspecting-data-structures",
    "title": "Reading in and inspecting data",
    "section": "Inspecting data structures",
    "text": "Inspecting data structures\nThere are a wide selection of base functions in R that are useful for inspecting your data and summarizing it. Let’s use the metadata file that we created to test out data inspection functions.\nTake a look at the dataframe by typing out the variable name metadata and pressing return; the variable contains information describing the samples in our study. Each row holds information for a single sample, and the columns contain categorical information about the sample genotype(WT or KO), celltype (typeA or typeB), and replicate number (1,2, or 3).\nmetadata\n\ngenotype celltype replicate\nsample1        Wt    typeA      1\nsample2        Wt    typeA      2\nsample3        Wt    typeA      3\nsample4        KO    typeA      1\nsample5        KO    typeA      2\nsample6        KO    typeA      3\nsample7        Wt    typeB      1\nsample8        Wt    typeB      2\nsample9        Wt    typeB      3\nsample10       KO    typeB      1\nsample11       KO    typeB      2\nsample12       KO    typeB      3\nSuppose we had a larger file, we might not want to display all the contents in the console. Instead we could check the top (the first 6 lines) of this data.frame using the function head():\nhead(metadata)\n\nList of functions for data inspection\nWe already saw how the functions head() and str() (in the releveling section) can be useful to check the content and the structure of a data.frame. Below is a non-exhaustive list of functions to get a sense of the content/structure of data. The list has been divided into functions that work on all types of objects, some that work only on vectors/factors (1 dimensional objects), and others that work on data frames and matrices (2 dimensional objects).\nWe have some exercises below that will allow you to gain more familiarity with these. You will definitely be using some of them in the next few homework sections.\n\nAll data structures - content display:\n\nstr(): compact display of data contents (similar to what you see in the Global environment)\n\nclass(): displays the data type for vectors (e.g. character, numeric, etc.) and data structure for dataframes, matrices, lists\nsummary(): detailed display of the contents of a given object, including descriptive statistics, frequencies\nhead(): prints the first 6 entries (elements for 1-D objects, rows for 2-D objects)\ntail(): prints the last 6 entries (elements for 1-D objects, rows for 2-D objects)\nVector and factor variables:\n\nlength(): returns the number of elements in a vector or factor\n\nDataframe and matrix variables:\n\ndim(): returns dimensions of the dataset (number_of_rows, number_of_columns) [Note, row numbers will always be displayed before column numbers in R]\n\nnrow(): returns the number of rows in the dataset\nncol(): returns the number of columns in the dataset\nrownames(): returns the row names in the dataset\n\ncolnames(): returns the column names in the dataset\n\n\nExercise 2\n\nUse the class() function on glengths and metadata, how does the output differ between the two?\nUse the summary() function on the proj_summary dataframe, what is the median “rRNA_rate”?\nHow long is the samplegroup factor?\nWhat are the dimensions of the proj_summary dataframe?\nWhen you use the rownames() function on metadata, what is the data structure of the output?\n[Optional] How many elements in (how long is) the output of colnames(proj_summary)? Don’t count, but use another function to determine this."
  },
  {
    "objectID": "get-started/5-data.html#section",
    "href": "get-started/5-data.html#section",
    "title": "Reading in and inspecting data",
    "section": "***",
    "text": "***\nThis lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n\nThe materials used in this lesson are adapted from work that is Copyright © Data Carpentry (http://datacarpentry.org/). All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0)."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html",
    "href": "sessions/week2/2-datawragling.html",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#learning-objectives",
    "href": "sessions/week2/2-datawragling.html#learning-objectives",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\n\nResearch Methods\nData Analyses\n\n\n\n\nReflect on how to make good research questions\nManipulate datasets as needed\n\n\n\nUse Tidyverse R package\n\n\n\nProduce simple tables and summaries"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#prerequisites",
    "href": "sessions/week2/2-datawragling.html#prerequisites",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you begin, ensure you have the following installed:\n\nR: You can download the latest version from the official R website. Instructions for different operating systems (Windows, Mac OS, UNIX/Linux) are available [8]. R must be installed before installing RStudio [9].\nRStudio: Download and install RStudio Desktop from https://www.rstudio.com/ [10]. To open RStudio, locate it in your applications and launch it [11].\ntidyverse package: Install the tidyverse package within R. Open RStudio and in the Console pane, type and run the following command [12]:\ninstall.packages(\"tidyverse\")\n\nThis command needs to be run only the first time you want to use the tidyverse. You can also install packages by navigating to Tools -&gt; Install Packages in RStudio."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#readings",
    "href": "sessions/week2/2-datawragling.html#readings",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Readings",
    "text": "Readings\n\n Check chapters 1 to 3 of the e-book Tidyverse Skills for Data Science\n Data tidying with tidyr :: Cheatsheet\n Data transformation with dplyr :: Cheatsheet"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#learning-objectives-1",
    "href": "sessions/week2/2-datawragling.html#learning-objectives-1",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDemonstrate how to subset, merge, and create new datasets from existing data structures in R.\nPerform basic data wrangling with functions in the Tidyverse package."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#tidyverse-basics",
    "href": "sessions/week2/2-datawragling.html#tidyverse-basics",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Tidyverse basics",
    "text": "Tidyverse basics\nThe Tidyverse suite of packages introduces users to a set of data structures, functions and operators to make working with data more intuitive, but is slightly different from the way we do things in base R. Two important new concepts we will focus on are pipes and tibbles.\nBefore we get started with pipes or tibbles, let’s load the library:\nlibrary(tidyverse)\n\nPipes\nStringing together commands in R can be quite daunting. Also, trying to understand code that has many nested functions can be confusing.\nTo make R code more human readable, the Tidyverse tools use the pipe, %&gt;%, which was acquired from the magrittr package and is now part of the dplyr package that is installed automatically with Tidyverse. The pipe allows the output of a previous command to be used as input to another command instead of using nested functions.\n\nNOTE: Shortcut to write the pipe is shift + command/ctrl + M\n\nAn example of using the pipe to run multiple commands:\n## A single command\nsqrt(83)\n\n## Base R method of running more than one command\nround(sqrt(83), digits = 2)\n\n## Running more than one command with piping\nsqrt(83) %&gt;% round(digits = 2)\nThe pipe represents a much easier way of writing and deciphering R code, and so we will be taking advantage of it, when possible, as we work through the remaining lesson.\n\nExercises\n\nCreate a vector of random numbers using the code below:\nrandom_numbers &lt;- c(81, 90, 65, 43, 71, 29)\nUse the pipe (%&gt;%) to perform two steps in a single line:\n\nTake the mean of random_numbers using the mean() function.\nRound the output to three digits using the round() function.\n\n\n\n\n\nTibbles\nA core component of the tidyverse is the tibble. Tibbles are a modern rework of the standard data.frame, with some internal improvements to make code more reliable. They are data frames, but do not follow all of the same rules. For example, tibbles can have numbers/symbols for column names, which is not normally allowed in base R.\nImportant: tidyverse is very opininated about row names. These packages insist that all column data (e.g. data.frame) be treated equally, and that special designation of a column as rownames should be deprecated. Tibble provides simple utility functions to handle rownames: rownames_to_column() and column_to_rownames().\nTibbles can be created directly using the tibble() function or data frames can be converted into tibbles using as_tibble(name_of_df).\n\nNOTE: The function as_tibble() will ignore row names, so if a column representing the row names is needed, then the function rownames_to_column(name_of_df) should be run prior to turning the data.frame into a tibble. Also, as_tibble() will not coerce character vectors to factors by default."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#experimental-data",
    "href": "sessions/week2/2-datawragling.html#experimental-data",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Experimental data",
    "text": "Experimental data\nWe’re going to explore the Tidyverse suite of tools to wrangle our data to prepare it for visualization. You should have downloaded the file called gprofiler_results_Mov10oe.tsv into your R project’s data folder earlier.\n\nIf you do not have the gprofiler_results_Mov10oe.tsv file in your data folder, you can right click and download it into the data folder using this link.\n\nThe dataset:\n\nRepresents the functional analysis results, including the biological processes, functions, pathways, or conditions that are over-represented in a given list of genes.\nOur gene list was generated by differential gene expression analysis and the genes represent differences between control mice and mice over-expressing a gene involved in RNA splicing.\n\nThe functional analysis that we will focus on involves gene ontology (GO) terms, which:\n\ndescribe the roles of genes and gene products\norganized into three controlled vocabularies/ontologies (domains):\n\nbiological processes (BP)\ncellular components (CC)\nmolecular functions (MF)"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#analysis-goal-and-workflow",
    "href": "sessions/week2/2-datawragling.html#analysis-goal-and-workflow",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Analysis goal and workflow",
    "text": "Analysis goal and workflow\nGoal: Visually compare the most significant biological processes (BP) based on the number of associated differentially expressed genes (gene ratios) and significance values by creating the following plot:\n\n\n\ndotplot6\n\n\nTo wrangle our data in preparation for the plotting, we are going to use the Tidyverse suite of tools to wrangle and visualize our data through several steps:\n\nRead in the functional analysis results\nExtract only the GO biological processes (BP) of interest\nSelect only the columns needed for visualization\nOrder by significance (p-adjusted values)\nRename columns to be more intuitive\nCreate additional metrics for plotting (e.g. gene ratios)\nPlot results"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#tidyverse-tools",
    "href": "sessions/week2/2-datawragling.html#tidyverse-tools",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Tidyverse tools",
    "text": "Tidyverse tools\nWhile all of the tools in the Tidyverse suite are deserving of being explored in more depth, we are going to investigate more deeply the reading (readr), wrangling (dplyr), and plotting (ggplot2) tools."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#read-in-the-functional-analysis-results",
    "href": "sessions/week2/2-datawragling.html#read-in-the-functional-analysis-results",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "1. Read in the functional analysis results",
    "text": "1. Read in the functional analysis results\nWhile the base R packages have perfectly fine methods for reading in data, the readr and readxl Tidyverse packages offer additional methods for reading in data. Let’s read in our tab-delimited functional analysis results using read_delim():\n# Read in the functional analysis results\nfunctional_GO_results &lt;- read_delim(file = \"data/gprofiler_results_Mov10oe.tsv\", delim = \"\\t\" )\n\n# Take a look at the results\nfunctional_GO_results\n\n\nClick here to see how to do this in base R\n\n\n# Read in the functional analysis results\nfunctional_GO_results &lt;- read.delim(file = \"data/gprofiler_results_Mov10oe.tsv\", sep = \"\\t\" )\n# Take a look at the results\nfunctional_GO_results\n\n\nNotice that the results were automatically read in as a tibble and the output gives the number of rows, columns and the data type for each of the columns.\n\nNOTE: A large number of tidyverse functions will work with both tibbles and dataframes, and the data structure of the output will be identical to the input. However, there are some functions that will return a tibble (without row names), whether or not a tibble or dataframe is provided."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#extract-only-the-go-biological-processes-bp-of-interest",
    "href": "sessions/week2/2-datawragling.html#extract-only-the-go-biological-processes-bp-of-interest",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "2. Extract only the GO biological processes (BP) of interest",
    "text": "2. Extract only the GO biological processes (BP) of interest\nNow that we have our data, we will need to wrangle it into a format ready for plotting. For all of our data wrangling steps we will be using tools from the dplyr package, which is a swiss-army knife for data wrangling of data frames.\nTo extract the biological processes of interest, we only want those rows where the domain is equal to BP, which we can do using the filter() function.\nTo filter rows of a data frame/tibble based on values in different columns, we give a logical expression as input to the filter() function to return those rows for which the expression is TRUE.\nNow let’s return only those rows that have a domain of BP:\n# Return only GO biological processes\nbp_oe &lt;- functional_GO_results %&gt;%\n  filter(domain == \"BP\")\n  \nView(bp_oe)\n\n\nClick here to see how to do this in base R\n\n\n# Return only GO biological processes\nidx &lt;- functional_GO_results$domain == \"BP\"\nbp_oe2 &lt;- functional_GO_results[idx,]\nView(bp_oe)\n\n\nNow we have returned only those rows with a domain of BP. How have the dimensions of our results changed?\n\nExercise:\nWe would like to perform an additional round of filtering to only keep the most specific GO terms.\n\nFor bp_oe, use the filter() function to only keep those rows where the relative.depth is greater than 4.\nSave output to overwrite our bp_oe variable."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#select-only-the-columns-needed-for-visualization",
    "href": "sessions/week2/2-datawragling.html#select-only-the-columns-needed-for-visualization",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "3. Select only the columns needed for visualization",
    "text": "3. Select only the columns needed for visualization\nFor visualization purposes, we are only interested in the columns related to the GO terms, the significance of the terms, and information about the number of genes associated with the terms.\nTo extract columns from a data frame/tibble we can use the select() function. In contrast to base R, we do not need to put the column names in quotes for selection.\n# Selecting columns to keep\nbp_oe &lt;- bp_oe %&gt;%\n  select(term.id, term.name, p.value, query.size, term.size, overlap.size, intersection)\n\nView(bp_oe)\n\n\nClick here to see how to do this in base R\n\n\n# Selecting columns to keep\nbp_oe &lt;- bp_oe[, c(\"term.id\", \"term.name\", \"p.value\", \"query.size\", \"term.size\", \"overlap.size\", \"intersection\")]\nView(bp_oe)\n\n\nThe select() function also allows for negative selection. So we could have alternately removed columns with negative selection. Note that we need to put the column names inside of the combine (c()) function with a - preceding it for this functionality.\n# DO NOT RUN\n# Selecting columns to remove\nbp_oe &lt;- bp_oe %&gt;%\n    select(-c(query.number, significant, recall, precision, subgraph.number, relative.depth, domain))\n\n\nClick here to see how to do this in base R\n\n\n# DO NOT RUN\n# Selecting columns to remove\nidx &lt;- !(colnames(bp_oe) %in% c(\"query.number\", \"significant\", \"recall\", \"precision\", \"subgraph.number\", \"relative.depth\", \"domain\"))\nbp_oe &lt;- bp_oe[, idx]"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#order-go-processes-by-significance-adjusted-p-values",
    "href": "sessions/week2/2-datawragling.html#order-go-processes-by-significance-adjusted-p-values",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "4. Order GO processes by significance (adjusted p-values)",
    "text": "4. Order GO processes by significance (adjusted p-values)\nNow that we have only the rows and columns of interest, let’s arrange these by significance, which is denoted by the adjusted p-value.\nLet’s sort the rows by adjusted p-value with the arrange() function.\n# Order by adjusted p-value ascending\nbp_oe &lt;- bp_oe %&gt;%\n  arrange(p.value)\n\n\nClick here to see how to do this in base R\n\n\n# Order by adjusted p-value ascending\nidx &lt;- order(bp_oe$p.value)\nbp_oe &lt;- bp_oe[idx,]\n\n\n\nNOTE1: If you wanted to arrange in descending order, then you could have run the following instead:\n# DO NOT RUN\n# Order by adjusted p-value descending\nbp_oe &lt;- bp_oe %&gt;%\n  arrange(desc(p.value))\n\n\nClick here to see how to do this in base R\n\n\n# DO NOT RUN\n# Order by adjusted p-value descending\nidx &lt;- order(bp_oe$p.value, decreasing = TRUE)\nbp_oe &lt;- bp_oe[idx,]\n\n\n\nNOTE2: Ordering variables in ggplot2 is a bit different. This post introduces a few ways of ordering variables in a plot."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#rename-columns-to-be-more-intuitive",
    "href": "sessions/week2/2-datawragling.html#rename-columns-to-be-more-intuitive",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "5. Rename columns to be more intuitive",
    "text": "5. Rename columns to be more intuitive\nWhile not necessary for our visualization, renaming columns more intuitively can help with our understanding of the data using the rename() function. The syntax is new_name = old_name.\nLet’s rename the term.id and term.name columns.\n# Provide better names for columns\nbp_oe &lt;- bp_oe %&gt;% \n  dplyr::rename(GO_id = term.id, \n                GO_term = term.name)\n\n\nClick here to see how to do this in base R\n\n\n# Provide better names for columns\ncolnames(bp_oe)[colnames(bp_oe) == \"term.id\"] &lt;- \"GO_id\"\ncolnames(bp_oe)[colnames(bp_oe) == \"term.name\"] &lt;- \"GO_term\"\n\n\n\nNOTE: In the case of two packages with identical function names, you can use :: with the package name before and the function name after (e.g stats::filter()) to ensure that the correct function is implemented. The :: can also be used to bring in a function from a library without loading it first.\nIn the example above, we wanted to use the rename() function specifically from the dplyr package, and not any of the other packages (or base R) which may have the rename() function.\n\n\nExercise\nRename the intersection column to genes to reflect the fact that these are the DE genes associated with the GO process."
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "href": "sessions/week2/2-datawragling.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "6. Create additional metrics for plotting (e.g. gene ratios)",
    "text": "6. Create additional metrics for plotting (e.g. gene ratios)\nFinally, before we plot our data, we need to create a couple of additional metrics. The mutate() function enables you to create a new column from an existing column.\nLet’s generate gene ratios to reflect the number of DE genes associated with each GO process relative to the total number of DE genes.\n# Create gene ratio column based on other columns in dataset\nbp_oe &lt;- bp_oe %&gt;%\n  mutate(gene_ratio = overlap.size / query.size)\n\n\nClick here to see how to do this in base R\n\n\n# Create gene ratio column based on other columns in dataset\nbp_oe &lt;- cbind(bp_oe, gene_ratio = bp_oe$overlap.size / bp_oe$query.size)\n\n\n\nExercise\nCreate a column in bp_oe called term_percent to determine the percent of DE genes associated with the GO term relative to the total number of genes associated with the GO term (overlap.size / term.size)\n\nOur final data for plotting should look like the table below:"
  },
  {
    "objectID": "sessions/week2/2-datawragling.html#next-steps",
    "href": "sessions/week2/2-datawragling.html#next-steps",
    "title": "Week 2 - Data Wrangling & Research Questions",
    "section": "Next steps",
    "text": "Next steps\nNow that we have our results ready for plotting, we can use the ggplot2 package to plot our results. If you are interested, you can follow this lesson and dive into how to use ggplot2 to create the plots with this dataset.\n\nAdditional resources\n\nR for Data Science\nteach the tidyverse\ntidy style guide\n\n\nThis lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  }
]