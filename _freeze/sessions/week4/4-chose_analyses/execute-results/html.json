{
  "hash": "7c8f2a366d67dff72886d0833bc4cfba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analysing Categorical Data - Frequency Tests and Contingency Tables\"\nsubtitle: \"Using R for Chi-Square Tests\"\nauthor: \"Your Name/Course Name\"\ndate: \"2023-10-27\"\nformat: html\neditor: visual\n---\n\n\n\n# Introduction to Categorical Data Analysis\n\nWhen working with data, we often encounter variables that represent categories rather than numerical quantities. These are known as **categorical variables** [1, 2]. Examples might include eye colour (blue, brown, green), habitat type (rural, industrial), or survival status (yes, no) [1, 3, 4]. Understanding the frequency distribution of a single categorical variable is straightforward, but researchers are frequently interested in the relationship or association between *two or more* categorical variables [1, 5].\n\nFor example, we might want to know if there is a relationship between eye colour and sex in a group of students, or whether the frequency of different ladybird colour forms varies between different habitats [1, 3]. To explore these relationships and test for associations, we use tools like contingency tables and the Chi-Square contingency table test [1, 2, 5, 6].\n\n# Contingency Tables\n\nA **contingency table** is a fundamental tool for summarising the frequency distribution of two or more categorical variables simultaneously [2, 5]. It organises the data into a grid where each cell shows the count of observations that fall into a specific combination of categories from the variables [5].\n\nA table summarising the frequencies of two categorical variables is called a **two-way contingency table** [5]. If more than two variables were involved, it would be a higher-dimensional table (e.g., a three-way contingency table for three variables) [5].\n\nThe term \"contingency\" refers to the table's ability to capture how the frequencies of one variable are 'contingent' upon, or associated with, the categories of another variable [5]. This association, linkage, non-independence, or interaction is evident when the proportions of observations in one set of categories depend on a second set of categories [5, 7].\n\nConsider two possibilities for a two-way table with categories R1, R2 and C1, C2:\n\n|       | C1 | C2 |\n|-------|----|----|\n| **R1**  | 10 | 20 |\n| **R2**  | 40 | 80 |\n\n|       | C1 | C2 |\n|-------|----|----|\n| **R1**  | 10 | 80 |\n| **R2**  | 40 | 20 |\n\nIn the first table (left), there is **no association**. The proportion of R1 cases relative to R2 cases is the same for both C1 and C2 (10/40 = 20/80 = 1/4). The total numbers in each category are irrelevant; it's the proportions that indicate association [7].\n\nIn the second table (right), there **is evidence of an association**. The proportion of R1 cases relative to R2 cases changes markedly between C1 (10/40 = 1/4) and C2 (80/20 = 4) [6]. The R1 cases are less frequent in the C1 column compared to the C2 column, relative to R2 [6]. Again, the proportions, not the raw numbers, reveal the association [6].\n\n## Creating Contingency Tables in R\n\nThe sources discuss several ways to create contingency tables in R, depending on how the raw data is structured [8-21].\n\nThe primary function for creating contingency tables from raw data is `table()` or `xtabs()` [8, 11, 19]. The `xtabs()` function is particularly useful as it can handle different data formats and is required for the `chisq.test()` function [8, 9].\n\n### Case 1: One Observation per Row\n\nWhen your data has one row for each individual observation and columns for each categorical variable, you can use `xtabs()` with a formula [9, 11].\n\nLet's use the `penguins` dataset as an example. Suppose we want a contingency table of `species` and `island`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the penguins data (assuming it's available, e.g., from the palmerpenguins package)\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAnexando pacote: 'palmerpenguins'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOs seguintes objetos s√£o mascarados por 'package:datasets':\n\n    penguins, penguins_raw\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(penguins)\n\n# Remove rows with missing values for species or island\npenguins_complete <- na.omit(penguins[, c(\"species\", \"island\")])\n\n# Create the contingency table using xtabs\nspecies_island_table <- xtabs(~ species + island, data = penguins_complete)\nspecies_island_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           island\nspecies     Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n```\n\n\n:::\n:::\n\n\n\nHere, `xtabs()` counts the number of occurrences for each combination of species and island.\nAlternatively, the `table()` function can be used, which is often simpler for this data structure.\n\n# Create the contingency table using table\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspecies_island_table_table <- table(penguins_complete$species, penguins_complete$island)\nspecies_island_table_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n```\n\n\n:::\n:::\n\n\n\nNotice that `table()` requires you to specify the columns using the $ operator, while `xtabs()` uses a formula.\nCase 2: Partially Summarised Data (Counts per Site/Group)\nSometimes data is partially summarised, for example, counts within different sub-locations. In this case, you'll have a column containing the counts and columns identifying the categories. You use `xtabs()` with the count variable on the left side of the formula `~`.\n\nLet's simulate a similar structure using mtcars, grouping by cylinder (cyl) and transmission type (am), and having a 'count' variable. This isn't a natural fit for mtcars, but we can illustrate the concept.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate partially summarised data from mtcars (e.g., counts per car type group)\n# This is for illustration of the data format, not a real mtcars use case\nmtcars_summary <- aggregate(mpg ~ cyl + am, data = mtcars, FUN = length)\n\n# Use xtabs with the count variable on the left\ncyl_am_table_summary <- xtabs(mpg ~ cyl + am, data = mtcars_summary)\ncyl_am_table_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   am\ncyl  0  1\n  4  3  8\n  6  4  3\n  8 12  2\n```\n\n\n:::\n:::\n\n\n\n\nIn this structure, xtabs(mpg ~ cyl + am, ...) tells R to sum the `mpg` variable for each combination of cyl and am.\n\nCase 3: Fully Summarised Data\nIf your data is already in a summarised format, with each row representing a unique combination of categories and a column for the total count for that combination, xtabs() can still be used. The syntax is the same as Case 2, with the count variable on the left side of ~. xtabs will essentially 'sum' over the single count for each combination, effectively converting the data frame format into an xtabs contingency table object, which is needed for `chisq.test()`.\nLet's use the ladybird example table:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual creation of the ladybird table as a data frame (similar to LADYBIRDS3.CSV)\nladybird_df_summarised <- data.frame(\n  Habitat = c(\"Industrial\", \"Industrial\", \"Rural\", \"Rural\"),\n  Colour = c(\"Black\", \"Red\", \"Black\", \"Red\"),\n  Number = c(115, 85, 30, 70)\n)\nladybird_df_summarised\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Habitat Colour Number\n1 Industrial  Black    115\n2 Industrial    Red     85\n3      Rural  Black     30\n4      Rural    Red     70\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use xtabs to convert to a contingency table object\nladybird_table_xtabs <- xtabs(Number ~ Habitat + Colour, data = ladybird_df_summarised)\nladybird_table_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Colour\nHabitat      Black Red\n  Industrial   115  85\n  Rural         30  70\n```\n\n\n:::\n:::\n\n\n\n\nThis produces the desired contingency table object.\n## Visualizing Contingency Tables\nVisualising contingency tables can provide quick insights into potential associations. Mosaic plots are particularly useful as they represent combinations of categories by rectangles whose size is proportional to the count, and can use colour to indicate deviation from expected counts under independence.\nUsing the penguins species by island table:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(graphics) # mosaicplot is in the built-in graphics package\n# Plot the table\nmosaicplot(species_island_table, shade = TRUE, las=2, main = \"Penguin Species by Island\")\n```\n\n::: {.cell-output-display}\n![](4-chose_analyses_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nIn a mosaic plot with `shade = TRUE`, blue colours indicate that the observed cell count is higher than expected under independence, while red indicates it is lower than expected. This helps identify which specific category combinations contribute most to any detected association. For example, the plot shows a strong positive association (blue) between Adelie penguins and Biscoe island, and Gentoo penguins and Torgersen island.\n## Chi-Square Contingency Table Test\nThe Pearson's Chi-Square ((\\chi^2)) contingency table test is the most widely used method to evaluate whether there is a significant association between two or more categorical variables summarised in a contingency table.\n\nHow the Test Works?\nThe core idea of the Chi-Square test is to compare the observed frequencies in the contingency table (the actual counts from the data) to the expected frequencies that would be observed if the variables were truly independent (i.e., if there was no association).\nThe test addresses the question of dependence between different kinds of outcomes or events by setting up a null hypothesis.\n‚Ä¢ Null Hypothesis ((H_0)): The row and column variables (the two categorical variables) are independent of one another. The occurrence of one event does not depend on the other.\n‚Ä¢ Alternative Hypothesis ((H_1)): The row and column variables are dependent; there is a significant association between the categories.\n\nStep 1: Calculate Expected Frequencies\nUnder the null hypothesis of independence, the expected number of observations in each cell of the table is calculated. The logic is based on probability: if two events are independent, the probability of both occurring is the product of their individual probabilities.\nFor any cell in the table, the expected value is calculated as: $$ E = \\frac{(\\text{Row Total}) \\times (\\text{Column Total})}{\\text{Grand Total}} $$\nLet's calculate the expected values for the ladybird colour by habitat table:\nBlack\nRed\nTotals\nRural\n30\n70\n100\nIndustrial\n115\n85\n200\nTotals\n145\n155\n300\n‚Ä¢ Expected (Rural, Black) = (\\frac{100 \\times 145}{300} \\approx 48.33)\n‚Ä¢ Expected (Rural, Red) = (\\frac{100 \\times 155}{300} \\approx 51.67)\n‚Ä¢ Expected (Industrial, Black) = (\\frac{200 \\times 145}{300} \\approx 96.67)\n‚Ä¢ Expected (Industrial, Red) = (\\frac{200 \\times 155}{300} \\approx 103.33)\n\nCompare these to the observed counts (30, 70, 115, 85). There are differences, but are they large enough to be statistically significant?\n\nStep 2: Calculate the Chi-Square Test Statistic\nThe Chi-Square test statistic ((\\chi^2)) quantifies the overall discrepancy between the observed counts ((o)) and the expected counts ((e)) across all cells in the table. The formula is: $$ \\chi^2 = \\sum{\\frac{(o - e)^2}{e}} $$\nThe sum is taken over all cells in the contingency table. A larger (\\chi^2) value indicates a greater mismatch between observed and expected counts, suggesting stronger evidence against the null hypothesis of independence.\n\nFor a 2x2 table, there is a shortcut formula, but it doesn't show the expected values: $$ \\chi^{2}=\\frac{k(bc-ad)^{2}}{efgh} $$ where k is the grand total, a, b, c, d are the cell counts, and e, f, g, h are the row and column totals.\nStep 3: Determine the p-value\nThe calculated (\\chi^2) statistic is compared to a theoretical Chi-Square distribution to determine the p-value. The p-value is the probability of observing a (\\chi^2) statistic as large as, or larger than, the one calculated from the data, assuming the null hypothesis of no association is true.\n\n‚Ä¢ A low p-value (typically less than a chosen significance level like 0.05) indicates that the observed pattern is unlikely under the null hypothesis, providing evidence against the null hypothesis and suggesting a significant association.\n‚Ä¢ A high p-value suggests the observed pattern is reasonably likely under the null hypothesis, meaning we fail to reject the null hypothesis; there is insufficient evidence to conclude a significant association.\n\nThe Chi-Square distribution used depends on the degrees of freedom (df), which for a two-way contingency table is calculated as ((r-1) \\times (c-1)), where (r) is the number of rows and (c) is the number of columns. For a 2x2 table, df = (2-1) * (2-1) = 1.\n\n## Assumptions of the Chi-Square Test\nLike most statistical tests, the Chi-Square contingency test has assumptions.\n1. Independent Observations: The counts in each cell must be independent. This means each observation should only contribute to one cell in the table, and the observations should not be related (e.g., repeated measurements on the same individuals are not independent).\n2. Sufficiently Large Expected Counts: The expected counts (not the observed counts) should not be too low. A common rule of thumb is that all expected values should be greater than 5. If this assumption is violated, the p-values generated by the test can become unreliable. In such cases, Fisher's exact test is a more appropriate alternative.\n\n## Performing the Chi-Square Test in R\nThe `chisq.test()` function in R performs the Chi-Square contingency table test. Its primary input is a contingency table object (like those created by `xtabs()` or `table()`).\nLet's perform the test on the ladybird data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ladybird_table_xtabs was created earlier using xtabs\nchisq.test(ladybird_table_xtabs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  ladybird_table_xtabs\nX-squared = 19.103, df = 1, p-value = 1.239e-05\n```\n\n\n:::\n:::\n\n\n\n\nThe output provides the test reminder, data used, the calculated (\\chi^2) statistic (X-squared), the degrees of freedom (df), and the p-value.\n\nNotice the output mentions \"Yates' continuity correction\". This correction is applied by default in R for 2x2 tables to provide more reliable p-values under certain circumstances. You can turn it off using correct = FALSE, but the default is generally safer for 2x2 tables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performing the test without Yates' correction (not recommended for 2x2)\nchisq.test(ladybird_table_xtabs, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  ladybird_table_xtabs\nX-squared = 20.189, df = 1, p-value = 7.015e-06\n```\n\n\n:::\n:::\n\n\n\n\nThe results are similar but slightly different. Stick with the default correct = TRUE for 2x2 tables.\nInterpreting the Output and Nature of Association\nThe p-value from the `chisq.test()` tells you whether there is a statistically significant association between the variables. A p-value less than your chosen significance level (e.g., 0.05) leads you to reject the null hypothesis and conclude that there is a significant association.\n\nFor the ladybird example, the p-value was very small (1.239e-05 with correction), which is much less than 0.05. We reject the null hypothesis and conclude there is a significant association between ladybird colour type and habitat.\nMerely stating that an association exists isn't always sufficient. We should also describe the nature of the association. For 2x2 tables, this is relatively easy: black individuals are more likely to be found in industrial areas in the ladybird example. For larger tables, interpreting the association is harder. Visualisations (like mosaic plots) and examining the differences between observed and expected counts can help.\n\nA more formal way to investigate the nature of the association, especially for larger tables, is by examining the residuals for each cell. Pearson residuals (or standardized residuals) are calculated as: $$ r = \\frac{o - e}{\\sqrt{e}} $$\nCells with large absolute residuals contribute most to the overall (\\chi^2) statistic. The sign of the residual indicates the direction of the association:\n\n‚Ä¢ Positive residuals (often shown in blue in plots like corrplot) indicate an attraction or positive association between the corresponding row and column categories ‚Äì the observed count is higher than expected.\n‚Ä¢ Negative residuals (often shown in red) indicate a repulsion or negative association ‚Äì the observed count is lower than expected.\nYou can access the observed counts, expected counts, and residuals from the output of the `chisq.test()` function using the $ operator.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nladybird_test_result <- chisq.test(ladybird_table_xtabs)\n# Observed counts\nladybird_test_result$observed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Colour\nHabitat      Black Red\n  Industrial   115  85\n  Rural         30  70\n```\n\n\n:::\n\n```{.r .cell-code}\n# Expected counts\nround(ladybird_test_result$expected, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Colour\nHabitat      Black    Red\n  Industrial 96.67 103.33\n  Rural      48.33  51.67\n```\n\n\n:::\n\n```{.r .cell-code}\n# Pearson residuals\nround(ladybird_test_result$residuals, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Colour\nHabitat       Black    Red\n  Industrial  1.865 -1.804\n  Rural      -2.637  2.551\n```\n\n\n:::\n:::\n\n\n\n\n\n\nLooking at the residuals, the positive residuals for (Industrial, Black) and (Rural, Red) indicate these combinations occur more often than expected under independence, while (Rural, Black) and (Industrial, Red) occur less often than expected. This confirms the conclusion that black forms are more frequent in industrial areas relative to red forms, compared to rural areas.\nThe contribution of each cell to the total (\\chi^2) score can also be calculated ( (r^2 / \\chi^2) ) to see which cells drive the significant result.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate cell contributions in percentage\ncell_contributions <- 100 * ladybird_test_result$residuals^2 / ladybird_test_result$statistic\nround(cell_contributions, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Colour\nHabitat       Black    Red\n  Industrial 18.201 17.027\n  Rural      36.403 34.054\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize contributions (requires corrplot package)\n# install.packages(\"corrplot\")\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.92 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\ncorrplot(cell_contributions, is.cor = FALSE, main = \"Cell Contributions to Chi-Square\")\n```\n\n::: {.cell-output-display}\n![](4-chose_analyses_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThe plot shows that the Industrial/Black and Rural/Red cells contribute the most to the significant Chi-Square result.\nAccessing Test Results\nThe result of `chisq.test()` is a list containing several components. You can access these using the $ operator:\n‚Ä¢ statistic: The (\\chi^2) test statistic value.\n‚Ä¢ parameter: The degrees of freedom.\n‚Ä¢ p.value: The p-value of the test.\n‚Ä¢ observed: The observed contingency table.\n‚Ä¢ expected: The expected contingency table under (H_0).\n‚Ä¢ residuals: The Pearson residuals for each cell.\n\n## Fisher's Exact Test\nWhen the assumption of sufficiently large expected counts for the Chi-Square test is not met (e.g., expected counts less than 5), Fisher's exact test is a more reliable alternative. It calculates the exact probability of observing the given table (or a more extreme one) assuming independence, without relying on approximations.\n\nFisher's exact test is performed using the fisher.test() function in R. It also takes a contingency table as input.\nLet's use the Titanic data example from the sources.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually create the Titanic sex vs survival table\ntitanic_table <- as.table(matrix(c(156, 307, 708, 142), nrow = 2, byrow = TRUE))\ncolnames(titanic_table) <- c(\"no\", \"yes\")\nrownames(titanic_table) <- c(\"female\", \"male\")\ntitanic_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        no yes\nfemale 156 307\nmale   708 142\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check expected counts for Chi-Square assumption\nchisq.test(titanic_table)$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             no      yes\nfemale 304.6702 158.3298\nmale   559.3298 290.6702\n```\n\n\n:::\n\n```{.r .cell-code}\n# Expected counts are > 5 here, so Chi-Square is appropriate, but Fisher's is also fine and often preferred computationally.\n\n# Perform Fisher's exact test\nfisher.test(titanic_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  titanic_table\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07759301 0.13384845\nsample estimates:\nodds ratio \n 0.1021212 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Manually create the Titanic sex vs survival table\ntitanic_table <- as.table(matrix(c(156, 307, 708, 142), nrow = 2, byrow = TRUE))\ncolnames(titanic_table) <- c(\"no\", \"yes\")\nrownames(titanic_table) <- c(\"female\", \"male\")\ntitanic_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        no yes\nfemale 156 307\nmale   708 142\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check expected counts for Chi-Square assumption\nchisq.test(titanic_table)$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             no      yes\nfemale 304.6702 158.3298\nmale   559.3298 290.6702\n```\n\n\n:::\n\n```{.r .cell-code}\n# Expected counts are > 5 here, so Chi-Square is appropriate, but Fisher's is also fine and often preferred computationally.\n\n# Perform Fisher's exact test\nfisher.test(titanic_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  titanic_table\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07759301 0.13384845\nsample estimates:\nodds ratio \n 0.1021212 \n```\n\n\n:::\n:::\n\n\n\n\nThe output includes the p-value. It also conveniently provides an estimate of the odds ratio and its 95% confidence interval. The odds ratio is another way to quantify the strength and direction of the association.\n\nFor the Titanic data, the odds ratio is about 0.10, indicating the odds of a male surviving were about one-tenth the odds of a female surviving. The confidence interval gives a range of plausible values for this odds ratio in the population.\nFisher's exact test is particularly recommended for 2x2 tables with small expected counts, but the `fisher.test()` function in R can handle larger tables as well, although it won't calculate odds ratios for tables larger than 2x2.\n## Exercises\nApply what you've learned to explore associations in different datasets.\n### Exercise 1: Car Characteristics ()\nThe mtcars dataset contains various characteristics for different car models. We can treat some of these as categorical variables. Let's investigate the relationship between the number of cylinders (cyl) and the transmission type (am, where 0 = automatic, 1 = manual).\n1. Load the mtcars dataset.\n2. Create a contingency table showing the frequencies of cyl by am. \n3. Examine the expected counts for a Chi-Square test. Are the assumptions met (all expected values > 5)?\n4. Perform the appropriate hypothesis test (Chi-Square if assumptions met, otherwise Fisher's exact test) to determine if there is a significant association between the number of cylinders and transmission type.\n5. Interpret the results of the test, including the p-value and conclusion regarding the null hypothesis.\n6. Based on the observed counts or residuals/contributions, describe the nature of any significant association found. For example, are cars with more cylinders more likely to have automatic or manual transmission?\n\n### Exercise 2: Penguin Species and Sex ()\nThe penguins dataset contains data on penguins from different islands.\n1. Load the penguins dataset and ensure you handle any missing values in the species and sex columns.\n2. Create a contingency table showing the frequencies of species by sex.\n3. Create a mosaic plot of the contingency table, using shading (shade=TRUE) to visualise deviations from independence.\n4. Examine the expected counts for a Chi-Square test. Are the assumptions met?\n5. Perform the appropriate hypothesis test to determine if there is a significant association between penguin species and sex.\n6. Interpret the test results. If there is a significant association, use the mosaic plot or residuals to describe how the distribution of sexes differs among the species.\n\n### Exercise 3: Food Poisoning Investigation\n\nThis exercise revisits the classic epidemiology case. The data is available in the file \"oswego.csv\". Assume you have downloaded the data and set your working directory or know the full file path.\n1. Load the \"oswego.csv\" data into R.\n2. Using `table()` and `chisq.test()$expected`, calculate the expected values for a (\\chi^2) contingency test of the relationship between eating \"fruit salad\" and being \"ill\". Would it be legitimate to use a (\\chi^2) contingency analysis to test this association based on the expected counts? What test would be best to use? \n3. The foods suspected were spinach, baked ham, vanilla ice cream, and chocolate ice cream. Use fisher.test() to calculate an odds ratio for illness for each of these foods (comparing those who ate the food to those who did not). Which food has the largest odds ratio? \n4. For the food identified as most likely vehicle, report the 95% confidence interval of the odds ratio for illness. \n5.\nDraw a mosaic plot to illustrate the association between eating the likely vehicle food and becoming ill. \n\n### Exercise 4: Shufflebottoms vs. Walkers ()\nThis exercise investigates a quirky study on infant movement and last names. The data is in \"shufflebottoms.csv\". Assume the file is accessible.\n1. Load the \"shufflebottoms.csv\" data. This data is likely already summarised (counts). Create the contingency table using the appropriate xtabs() method for summarised data (Case 3). The table should compare Bottom-shuffling frequency between infants named \"Shufflebottom\" and \"Walker\". (Observed counts: 9 out of 41 Walkers bottom-shuffled; 9 out of 43 Shufflebottoms bottom-shuffled). \n2. Calculate the odds ratio for the association between name and bottom-shuffling. Provide a 95% confidence interval for this odds ratio. \n3. Based only on the 95% confidence interval for the odds ratio, would you expect a hypothesis test (like Fisher's exact test) to find a significant association? Why or why not? (Hint: Consider if the confidence interval includes 1, which represents no association).\n4. Perform Fisher's exact test to formally test for a significant difference in bottom-shuffling frequency between the two name groups. What is the p-value and your conclusion? Does this match your expectation from step 3? [Based on source 101]\n\n\n",
    "supporting": [
      "4-chose_analyses_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}